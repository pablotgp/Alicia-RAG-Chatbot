{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install -q python-dotenv"
      ],
      "metadata": {
        "id": "IFOV3J1xXiCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuN1gBeRXK2c"
      },
      "outputs": [],
      "source": [
        "# Ejemplo para cargar claves en un notebook\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "if not OPENAI_API_KEY or not GOOGLE_API_KEY:\n",
        "    print(\"⚠️ ADVERTENCIA: No se encontraron las claves de API en el entorno.\")\n",
        "    print(\"Asegúrate de tener un archivo .env o de haber configurado los secretos de Colab.\")\n",
        "else:\n",
        "    print(\"✅ Claves de API cargadas exitosamente.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Añade esto al final de tu script de generación de embeddings ---\n",
        "\n",
        "import numpy as np\n",
        "import faiss\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Suponiendo que 'chunks_final_data' es tu lista de chunks con texto, metadatos y embeddings\n",
        "\n",
        "# --- CONFIGURACIÓN DE NOMBRES DE ARCHIVO ---\n",
        "# Usemos nombres que correspondan a nuestro nuevo libro\n",
        "INDEX_PATH = \"alicia.index\"\n",
        "TEXTS_PATH = \"alicia_texts.pkl\"\n",
        "METAS_PATH = \"alicia_metas.pkl\"\n",
        "\n",
        "# 1. Separar los datos para guardarlos\n",
        "embeddings = np.array([chunk['embedding'] for chunk in chunks_final_data], dtype=np.float32)\n",
        "texts = [chunk['text'] for chunk in chunks_final_data]\n",
        "metadatas = [chunk['metadata'] for chunk in chunks_final_data]\n",
        "\n",
        "print(f\"Datos separados: {len(embeddings)} embeddings, {len(texts)} textos, {len(metadatas)} metadatos.\")\n",
        "print(f\"Dimensiones del vector de embedding: {embeddings.shape[1]}\")\n",
        "\n",
        "# 2. Crear y entrenar el índice FAISS\n",
        "# Usamos un índice simple 'IndexFlatL2' que es bueno para empezar\n",
        "d = embeddings.shape[1]  # Dimensión de los vectores\n",
        "index = faiss.IndexFlatL2(d)\n",
        "print(f\"Índice FAISS vacío creado con dimensión {d}.\")\n",
        "\n",
        "# Añadir los vectores al índice\n",
        "index.add(embeddings)\n",
        "print(f\"Se han añadido {index.ntotal} vectores al índice FAISS.\")\n",
        "\n",
        "# 3. Guardar todo en archivos\n",
        "print(f\"Guardando índice FAISS en '{INDEX_PATH}'...\")\n",
        "faiss.write_index(index, INDEX_PATH)\n",
        "\n",
        "print(f\"Guardando textos en '{TEXTS_PATH}'...\")\n",
        "with open(TEXTS_PATH, 'wb') as f:\n",
        "    pickle.dump(texts, f)\n",
        "\n",
        "print(f\"Guardando metadatos en '{METAS_PATH}'...\")\n",
        "with open(METAS_PATH, 'wb') as f:\n",
        "    pickle.dump(metadatas, f)\n",
        "\n",
        "print(\"\\n--- ¡Proceso de guardado completado! ---\")\n",
        "print(\"Ahora puedes usar estos 3 archivos en tu script de búsqueda RAG.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "doS-53Eewaw9",
        "outputId": "91a2a145-f511-43a1-a07b-b4e4a10e94b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datos separados: 143 embeddings, 143 textos, 143 metadatos.\n",
            "Dimensiones del vector de embedding: 1536\n",
            "Índice FAISS vacío creado con dimensión 1536.\n",
            "Se han añadido 143 vectores al índice FAISS.\n",
            "Guardando índice FAISS en 'alicia.index'...\n",
            "Guardando textos en 'alicia_texts.pkl'...\n",
            "Guardando metadatos en 'alicia_metas.pkl'...\n",
            "\n",
            "--- ¡Proceso de guardado completado! ---\n",
            "Ahora puedes usar estos 3 archivos en tu script de búsqueda RAG.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ocCTci55yI_",
        "outputId": "3fb788b2-9d19-4c4d-ec91-ed9ebe636b87",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Celda 2 - Parámetros y Variables Globales definidas.\n",
            "  - K_FAISS_INITIAL: 100\n",
            "  - K_BM25_INITIAL: 100\n",
            "  - K_RERANK: 80\n",
            "  - K_FINAL: 3\n",
            "  - RERANKER_MODEL: cross-encoder/ms-marco-MiniLM-L-12-v2\n",
            "--- Fin Celda 2 ---\n"
          ]
        }
      ],
      "source": [
        "# --- Celda 2: Parámetros de Configuración y Variables Globales ---\n",
        "\n",
        "# --- Parámetros de Archivos y Azure ---\n",
        "INDEX_PATH = \"alicia.index\"\n",
        "TEXTS_PATH = \"alicia_texts.pkl\"\n",
        "METAS_PATH = \"alicia_metas.pkl\"\n",
        "AZURE_EMBEDDING_DEPLOYMENT_NAME = \"text-embedding-3-small\" # Asegúrate que coincide con tu deployment\n",
        "AZURE_API_VERSION = \"2024-02-01\" # O la versión que uses ej: \"2023-05-15\"\n",
        "\n",
        "# --- Parámetros para Búsqueda Híbrida y Reranking ---\n",
        "K_FAISS_INITIAL = 100  # Número de candidatos a recuperar de FAISS\n",
        "K_BM25_INITIAL = 100   # Número de candidatos a recuperar de BM25\n",
        "K_RERANK = 80         # Número de candidatos a pasar al reranker (<= K_FAISS + K_BM25)\n",
        "K_FINAL = 3\n",
        "USE_DYNAMIC_K = True        # True para usar K dinámico, False para usar K_FINAL fijo\n",
        "RERANKER_SCORE_THRESHOLD = 1.5 # Umbral mínimo para considerar un chunk (ajustar según scores observados)\n",
        "MIN_CHUNKS_DYNAMIC = 3      # Mínimo de chunks a devolver si USE_DYNAMIC_K es True\n",
        "MAX_CHUNKS_DYNAMIC = 7      # Máximo de chunks a devolver si USE_DYNAMIC_K es True         # Número final de chunks a devolver al LLM            # Número final de chunks a devolver al LLM\n",
        "RERANKER_MODEL = 'cross-encoder/ms-marco-MiniLM-L-12-v2' # Modelo CrossEncoder\n",
        "\n",
        "# --- Variables Globales para inicialización única (se llenarán en la primera ejecución) ---\n",
        "is_retriever_initialized = False\n",
        "# Objetos principales:\n",
        "embeddings_model = None\n",
        "faiss_index = None\n",
        "texts = None\n",
        "metadatas = None\n",
        "bm25 = None\n",
        "reranker = None\n",
        "# Opcional: stop words en español si usas NLTK\n",
        "# spanish_stopwords = stopwords.words('spanish')\n",
        "\n",
        "print(\"INFO: Celda 2 - Parámetros y Variables Globales definidas.\")\n",
        "print(f\"  - K_FAISS_INITIAL: {K_FAISS_INITIAL}\")\n",
        "print(f\"  - K_BM25_INITIAL: {K_BM25_INITIAL}\")\n",
        "print(f\"  - K_RERANK: {K_RERANK}\")\n",
        "print(f\"  - K_FINAL: {K_FINAL}\")\n",
        "print(f\"  - RERANKER_MODEL: {RERANKER_MODEL}\")\n",
        "print(\"--- Fin Celda 2 ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWQVy_lU5yI_",
        "outputId": "cdf84a6c-a7c9-4be8-e2f1-51ad6b036238",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Celda 3 - Funciones auxiliares definidas (tokenizer, pesos, normalización).\n",
            "  - Usando tokenizer: simple_tokenizer\n",
            "--- Fin Celda 3 ---\n"
          ]
        }
      ],
      "source": [
        "# --- Celda 3: Funciones Auxiliares ---\n",
        "\n",
        "def simple_tokenizer(text):\n",
        "    \"\"\"Tokenizador simple: minúsculas y split por espacios.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    return text.lower().split()\n",
        "\n",
        "# Opcional: Tokenizador más robusto con NLTK (requiere descargas en Celda 1)\n",
        "# def nltk_tokenizer(text):\n",
        "#     \"\"\"Tokenizador con NLTK: minúsculas, palabras, sin puntuación ni stopwords.\"\"\"\n",
        "#     if not isinstance(text, str):\n",
        "#         return []\n",
        "#     words = word_tokenize(text.lower(), language='spanish')\n",
        "#     # Asegúrate que spanish_stopwords está definida si descomentas esto\n",
        "#     # return [word for word in words if word.isalnum() and word not in spanish_stopwords]\n",
        "#     return [word for word in words if word.isalnum()] # Sin stopwords\n",
        "\n",
        "# Elige tu tokenizador preferido aquí (¡asegúrate que la función existe!)\n",
        "tokenizer_for_bm25 = simple_tokenizer\n",
        "# tokenizer_for_bm25 = nltk_tokenizer # Si prefieres NLTK\n",
        "\n",
        "\n",
        "def norm_score(score, min_val, max_val):\n",
        "    \"\"\"\n",
        "    Normaliza un score a un rango [0, 1].\n",
        "    Maneja el caso donde min_val == max_val para evitar división por cero.\n",
        "    \"\"\"\n",
        "    if min_val == max_val:\n",
        "        # Si todos los scores son iguales, podemos devolver 0.5 (neutral) o 1 si el score es ese valor, o 0.\n",
        "        # Devolver 0 si min_val == max_val y score == min_val (o cualquier score ya que son todos iguales)\n",
        "        # o 0.5 para indicar que no hay varianza. Elegiremos 0.5 como un valor neutral.\n",
        "        # Otra opción es devolver 1.0 si solo hay un resultado y es positivo, o 0.0 si es 0.\n",
        "        # O, si solo hay un elemento, su score normalizado puede ser 1.\n",
        "        return 1.0 if score > 0 else 0.0 # Si hay un solo score y es > 0, es el \"mejor\"\n",
        "    if max_val - min_val == 0: # Otra forma de chequear división por cero\n",
        "        return 0.5 # O 1.0 si el score es el único valor\n",
        "    return (score - min_val) / (max_val - min_val)\n",
        "\n",
        "import re\n",
        "\n",
        "def calcular_pesos_dinamicos(query: str, subject: str = None) -> tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Analiza la query educativa y el tema (opcional) y ajusta pesos entre BM25 y Embeddings.\n",
        "    Devuelve (peso_bm25, peso_emb).\n",
        "    \"\"\"\n",
        "    query_lower = query.lower()\n",
        "    query_original = query # Para checks de mayúsculas\n",
        "\n",
        "    # --- Pesos Base ---\n",
        "    peso_bm25 = 0.4\n",
        "    peso_emb = 0.6\n",
        "    razon_principal = \"Default (ligero sesgo Embedding)\"\n",
        "    detalles_razon = []\n",
        "\n",
        "    # --- 1. Indicadores de ALTA ESPECIFICIDAD (Prioridad Alta para BM25) ---\n",
        "\n",
        "    # 1.1. Citas exactas (texto entre comillas)\n",
        "    if re.search(r'\"[^\"]+\"', query_original): # Busca texto entre comillas dobles\n",
        "        peso_bm25 = 0.85\n",
        "        peso_emb = 0.15\n",
        "        razon_principal = \"Cita Exacta\"\n",
        "        detalles_razon.append(\"BM25 priorizado para coincidencia literal.\")\n",
        "        print_pesos_info(razon_principal, detalles_razon, peso_bm25, peso_emb)\n",
        "        return peso_bm25, peso_emb\n",
        "\n",
        "    # 1.bis. Definición de Término Clave Específico (Ej: \"elipsis\", \"hipérbaton\")\n",
        "    definicion_keywords_specific_term = [\n",
        "        \"define\", \"definición de\", \"definir\", \"significa\",\n",
        "        \"qué es\", \"que es\", \"cuál es el significado de\",\n",
        "        \"concepto de\"\n",
        "    ]\n",
        "    term_to_define_specific = \"\"\n",
        "    for keyword in definicion_keywords_specific_term:\n",
        "        # Patrón para \"keyword X\" o \"keyword 'X'\" o \"keyword \"X\"\"\n",
        "        # o para \"X keyword\" (menos común para estas keywords pero podría pasar)\n",
        "        # Priorizamos \"keyword X\"\n",
        "        if query_lower.startswith(keyword + \" \"):\n",
        "            potential_term = query_lower[len(keyword)+1:].strip()\n",
        "            # Quitar comillas y signos de interrogación del término\n",
        "            potential_term = re.sub(r\"['\\\"?¿!¡]$\", \"\", potential_term).strip()\n",
        "            potential_term = re.sub(r\"^['\\\"]\", \"\", potential_term).strip()\n",
        "\n",
        "            # Si la query original tenía el término entre comillas, es buena señal\n",
        "            if f\"'{potential_term}'\" in query_original or f'\"{potential_term}\"' in query_original:\n",
        "                 term_to_define_specific = potential_term\n",
        "                 break\n",
        "            # Si no, tomarlo si es corto\n",
        "            elif len(potential_term.split()) <= 3:\n",
        "                 term_to_define_specific = potential_term\n",
        "                 break\n",
        "\n",
        "    if term_to_define_specific and len(term_to_define_specific.split()) <= 3 and len(query.split()) < 8 : # Término corto, query no demasiado larga\n",
        "        # Evitar que una pregunta conceptual larga que casualmente empieza con \"qué es la vida...\" caiga aquí\n",
        "        # Si la query es más larga, es probable que sea más conceptual.\n",
        "        peso_bm25 = 0.80 # Alta prioridad para BM25 para encontrar el término exacto\n",
        "        peso_emb = 0.20\n",
        "        razon_principal = \"Definición de Término Clave Específico\"\n",
        "        detalles_razon.append(f\"Término detectado: '{term_to_define_specific}'. BM25 fuertemente priorizado.\")\n",
        "        print_pesos_info(razon_principal, detalles_razon, peso_bm25, peso_emb)\n",
        "        return peso_bm25, peso_emb\n",
        "\n",
        "\n",
        "    # 1.2. Búsqueda de Leyes, Artículos, Teoremas específicos\n",
        "    if re.search(r'\\b(ley|artículo|teorema|postulado|axioma|principio)\\s+([0-9]+|[xviíclmd]+|[A-Za-z\\s]+)\\b', query_lower, re.IGNORECASE):\n",
        "        peso_bm25 = 0.75\n",
        "        peso_emb = 0.25\n",
        "        razon_principal = \"Ley/Artículo/Teorema Específico\"\n",
        "        detalles_razon.append(\"BM25 priorizado para identificadores exactos.\")\n",
        "        print_pesos_info(razon_principal, detalles_razon, peso_bm25, peso_emb)\n",
        "        return peso_bm25, peso_emb\n",
        "\n",
        "    # 1.3. Fórmulas o Ecuaciones\n",
        "    if re.search(r'\\b[a-zA-Z]\\s*=\\s*[a-zA-Z0-9]|\\b[a-zA-Z]\\w*\\([a-zA-Z\\d,\\s]*\\)|[a-zA-Z]\\w*_[a-zA-Z\\d]|\\w\\^[2-9]\\b', query_original):\n",
        "        if subject in [\"Física\", \"Biología\", \"Matemáticas\", \"Química\"]: # Más probable que sea una fórmula\n",
        "            peso_bm25 = 0.70\n",
        "            peso_emb = 0.30\n",
        "            razon_principal = \"Posible Fórmula/Ecuación\"\n",
        "            detalles_razon.append(f\"BM25 priorizado en {subject} para coincidencia estructural.\")\n",
        "            print_pesos_info(razon_principal, detalles_razon, peso_bm25, peso_emb)\n",
        "            return peso_bm25, peso_emb\n",
        "\n",
        "    # --- 2. Indicadores de ESPECIFICIDAD MEDIA (Favorecen BM25, pero con espacio para semántica) ---\n",
        "\n",
        "    # 2.1. Nombres Propios\n",
        "    nombres_propios_candidatos = re.findall(r'\\b[A-ZÁÉÍÓÚÑ][a-záéíóúñ]{2,}(?:\\s+[A-ZÁÉÍÓÚÑ][a-záéíóúñ]{1,})*\\b', query_original)\n",
        "    if nombres_propios_candidatos:\n",
        "        if not (len(nombres_propios_candidatos) == 1 and query_original.startswith(nombres_propios_candidatos[0]) and len(query.split()) > 3):\n",
        "            peso_bm25 = max(peso_bm25, 0.65) # Aumenta si el default era menor, o lo establece\n",
        "            peso_emb = 1.0 - peso_bm25\n",
        "            if razon_principal.startswith(\"Default\"): razon_principal = \"Nombre Propio Detectado\"\n",
        "            detalles_razon.append(f\"Candidatos NP: {nombres_propios_candidatos}. BM25 priorizado.\")\n",
        "\n",
        "    # 2.2. Fechas, Años, Siglos\n",
        "    if re.search(r'\\b\\d{3,4}\\b', query_lower) or \\\n",
        "       re.search(r'\\bsiglo\\s+(?:[xviíclmd]+|[0-9]+)\\b', query_lower) or \\\n",
        "       re.search(r'\\b(año|fecha)\\s+\\d{1,4}\\b', query_lower) or \\\n",
        "       re.search(r'\\b\\d{1,2}(?:/| de |-| del )\\w+(?:/| de |-| del )\\d{2,4}\\b', query_lower):\n",
        "        peso_bm25 = max(peso_bm25, 0.70)\n",
        "        peso_emb = 1.0 - peso_bm25\n",
        "        if razon_principal.startswith(\"Default\") or \"Nombre Propio\" in razon_principal: razon_principal = \"Fecha/Año/Siglo Detectado\"\n",
        "        detalles_razon.append(\"BM25 priorizado para especificidad temporal.\")\n",
        "        if subject == \"Historia\":\n",
        "            peso_bm25 = max(peso_bm25, 0.75) # Aún más para Historia\n",
        "            peso_emb = 1.0 - peso_bm25\n",
        "            detalles_razon.append(\"Alta prioridad BM25 en Historia.\")\n",
        "\n",
        "    # 2.3. Acrónimos y Términos Técnicos Muy Específicos\n",
        "    acronimos_candidatos = re.findall(r'\\b[A-ZÁÉÍÓÚÑ]{2,}\\b', query_original)\n",
        "    if acronimos_candidatos and not query_original.isupper():\n",
        "        if not (len(acronimos_candidatos) == 1 and query_original.startswith(acronimos_candidatos[0])):\n",
        "            peso_bm25 = max(peso_bm25, 0.60)\n",
        "            peso_emb = 1.0 - peso_bm25\n",
        "            if razon_principal.startswith(\"Default\") or \"Nombre Propio\" in razon_principal or \"Fecha\" in razon_principal:\n",
        "                razon_principal = \"Acrónimo/Término Técnico Específico Detectado\"\n",
        "            detalles_razon.append(f\"Candidatos Acrónimo: {acronimos_candidatos}. BM25 con peso incrementado.\")\n",
        "\n",
        "\n",
        "    # --- 3. Indicadores de BÚSQUEDA DE DEFINICIONES (Equilibrio, si no es ya muy específico) ---\n",
        "    # Esta regla se aplica si las de ALTA ESPECIFICIDAD (incluida 1.bis) no se activaron y retornaron.\n",
        "    definicion_keywords_general = [\"define\", \"definición de\", \"definir\", \"significa\", \"concepto de\"]\n",
        "    que_es_keywords_general = [\"qué es\", \"que es\", \"cual es el significado de\", \"cuál es el significado de\"]\n",
        "\n",
        "    is_general_definition_request = False\n",
        "    if any(keyword in query_lower for keyword in definicion_keywords_general) or \\\n",
        "       any(query_lower.startswith(keyword) for keyword in que_es_keywords_general):\n",
        "        is_general_definition_request = True\n",
        "\n",
        "    if is_general_definition_request:\n",
        "        # Si ya se marcó como muy específico (nombre propio, fecha, acrónimo), mantenemos BM25 alto,\n",
        "        # pero si la razón principal aún es \"Default\" o algo menos específico.\n",
        "        if peso_bm25 < 0.6: # Solo ajusta si no es ya específico por reglas anteriores\n",
        "            peso_bm25 = 0.55\n",
        "            peso_emb = 0.45\n",
        "            razon_principal = \"Petición de Definición General\"\n",
        "            detalles_razon.append(\"Pesos ligeramente inclinados a BM25 para literalidad, pero con semántica.\")\n",
        "        else:\n",
        "            detalles_razon.append(\"Petición de definición, pero query ya tenía especificidad media/alta.\")\n",
        "\n",
        "\n",
        "    # --- 4. Indicadores de CONCEPTUALIDAD (Prioridad para Embeddings) ---\n",
        "    concept_keywords_strong = [\"explica\", \"describe el proceso de\", \"analiza las causas de\", \"compara y contrasta\",\n",
        "                               \"cuál es la importancia de\", \"interpreta\", \"relación entre\", \"impacto de\",\n",
        "                               \"evolución de\", \"fundamentos de\", \"teoría de\"]\n",
        "    concept_keywords_medium = [\"cómo funciona\", \"por qué ocurre\", \"cuáles son las características\",\n",
        "                               \"tipos de\", \"función de\", \"origen de\", \"propiedades de\"]\n",
        "\n",
        "    is_conceptual = False\n",
        "    conceptual_keyword_found = \"\"\n",
        "    for keyword in concept_keywords_strong:\n",
        "        if keyword in query_lower:\n",
        "            is_conceptual = True\n",
        "            conceptual_keyword_found = keyword\n",
        "            detalles_razon.append(f\"Palabra clave conceptual fuerte detectada: '{keyword}'.\")\n",
        "            break\n",
        "    if not is_conceptual:\n",
        "        for keyword in concept_keywords_medium:\n",
        "            if keyword in query_lower:\n",
        "                is_conceptual = True\n",
        "                conceptual_keyword_found = keyword\n",
        "                detalles_razon.append(f\"Palabra clave conceptual media detectada: '{keyword}'.\")\n",
        "                break\n",
        "\n",
        "    if is_conceptual:\n",
        "        # Si es una pregunta conceptual sobre un término muy específico (ya capturado por NP, Fecha, Acrónimo)\n",
        "        # Ej: \"Explica el impacto de la Peste Negra\" -> Peste Negra (NP) + Explica (Conceptual)\n",
        "        if peso_bm25 >= 0.65 : # Ya era muy específico\n",
        "            peso_bm25 = 0.55 # Mantenemos algo de BM25 para el término, pero damos espacio a la explicación\n",
        "            peso_emb = 0.45\n",
        "            razon_principal = \"Pregunta Conceptual Muy Específica\"\n",
        "            detalles_razon.append(f\"Término específico combinado con petición conceptual ('{conceptual_keyword_found}').\")\n",
        "        elif peso_bm25 >= 0.55 and peso_bm25 < 0.65: # Especificidad media\n",
        "            peso_bm25 = 0.40\n",
        "            peso_emb = 0.60\n",
        "            razon_principal = \"Pregunta Conceptual con Especificidad Media\"\n",
        "            detalles_razon.append(f\"Término con especificidad media combinado con petición conceptual ('{conceptual_keyword_found}').\")\n",
        "        else: # Pregunta conceptual más general\n",
        "            peso_bm25 = 0.25\n",
        "            peso_emb = 0.75\n",
        "            razon_principal = \"Pregunta Conceptual General\"\n",
        "            detalles_razon.append(f\"Mayor peso para Embeddings debido a '{conceptual_keyword_found}'.\")\n",
        "\n",
        "\n",
        "    # --- 5. Ajustes por Asignatura (si se proporciona y no hay una regla fuerte dominante) ---\n",
        "    if subject and (razon_principal.startswith(\"Default\") or \"Petición de Definición General\" in razon_principal):\n",
        "        original_razon_principal = razon_principal # Guardar por si no se modifica\n",
        "        if subject == \"Lengua Castellana\":\n",
        "            if \"analiza el poema\" in query_lower or \"figuras retóricas\" in query_lower or \"estilo de\" in query_lower or \"comentario de texto\" in query_lower:\n",
        "                peso_bm25 = 0.3\n",
        "                peso_emb = 0.7\n",
        "                razon_principal = f\"Conceptual (Lengua - Análisis Literario)\"\n",
        "            elif \"regla gramatical\" in query_lower or \"ortografía de\" in query_lower or \"sintaxis de\" in query_lower:\n",
        "                peso_bm25 = 0.6\n",
        "                peso_emb = 0.4\n",
        "                razon_principal = f\"Específico (Lengua - Gramática/Ortografía)\"\n",
        "        elif subject == \"Historia\":\n",
        "            if \"batalla de\" in query_lower or \"tratado de\" in query_lower or \"reinado de\" in query_lower or \"guerra de\" in query_lower:\n",
        "                if peso_bm25 < 0.65: # Solo si no fue ya capturado por NP/Fecha con alta prioridad\n",
        "                    peso_bm25 = 0.65\n",
        "                    peso_emb = 0.35\n",
        "                    razon_principal = f\"Evento Específico (Historia)\"\n",
        "\n",
        "        if original_razon_principal != razon_principal: # Si se aplicó una regla de asignatura\n",
        "             detalles_razon.append(f\"Ajuste por asignatura '{subject}'.\")\n",
        "\n",
        "\n",
        "    # --- 6. Ajuste final por longitud de la query (si aún es default o poco definido) ---\n",
        "    # Se aplica si ninguna regla fuerte o de especificidad media/conceptual clara dominó\n",
        "    if razon_principal.startswith(\"Default\") or \\\n",
        "       (\"Petición de Definición General\" in razon_principal and peso_bm25 == 0.55) or \\\n",
        "       (peso_bm25 >= 0.35 and peso_bm25 <= 0.45 and not is_conceptual): # Default o ligeramente inclinado a Emb sin ser conceptual fuerte\n",
        "\n",
        "        num_words_query = len(query.split())\n",
        "        if num_words_query > 10:\n",
        "            peso_bm25 = 0.30\n",
        "            peso_emb = 0.70\n",
        "            razon_principal = \"Ajuste por Longitud (Larga -> Conceptual)\"\n",
        "            detalles_razon.append(f\"Query larga ({num_words_query} palabras), favoreciendo semántica.\")\n",
        "        elif num_words_query < 4:\n",
        "            peso_bm25 = 0.50 # Si era default (0.4), lo sube un poco para términos cortos\n",
        "            peso_emb = 0.50\n",
        "            razon_principal = \"Ajuste por Longitud (Corta -> Equilibrio/Específica)\"\n",
        "            detalles_razon.append(f\"Query corta ({num_words_query} palabras), buscando equilibrio o término.\")\n",
        "\n",
        "\n",
        "    print_pesos_info(razon_principal, detalles_razon, peso_bm25, peso_emb)\n",
        "    return peso_bm25, peso_emb\n",
        "\n",
        "def print_pesos_info(razon_principal, detalles_razon, peso_bm25, peso_emb):\n",
        "    \"\"\"Función auxiliar para imprimir la información de los pesos.\"\"\"\n",
        "    print(f\"  INFO DinamicWeights: Razón Principal = {razon_principal}\")\n",
        "    if detalles_razon:\n",
        "        for detalle in detalles_razon:\n",
        "            print(f\"    - {detalle}\")\n",
        "    print(f\"  INFO DinamicWeights: Pesos Asignados -> BM25={peso_bm25:.2f}, Embedding={peso_emb:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"INFO: Celda 3 - Funciones auxiliares definidas (tokenizer, pesos, normalización).\")\n",
        "print(f\"  - Usando tokenizer: {tokenizer_for_bm25.__name__}\")\n",
        "print(\"--- Fin Celda 3 ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic = 'Alicia en el pais de las maravillas'"
      ],
      "metadata": {
        "id": "q5EI_O0Kw55R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mo63UKyZ5yJA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Asumo que las importaciones necesarias como numpy, faiss, pickle, etc., ya están en tu archivo.\n",
        "# Asegúrate de importar la clase correcta:\n",
        "from rank_bm25 import BM25Okapi\n",
        "from langchain_openai import OpenAIEmbeddings # <--- CAMBIO: Importar esta clase\n",
        "from sentence_transformers import CrossEncoder\n",
        "# ... (resto de tus importaciones y variables globales como is_retriever_initialized)\n",
        "\n",
        "# Asumo que las importaciones y variables globales ya están definidas antes de esta función.\n",
        "# Librerías necesarias:\n",
        "# import faiss, pickle, os, traceback\n",
        "# import numpy as np\n",
        "# from rank_bm25 import BM25Okapi\n",
        "# from langchain_openai import OpenAIEmbeddings\n",
        "# from sentence_transformers import CrossEncoder\n",
        "\n",
        "def my_hybrid_rerank_retriever(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Función retriever completa que usa búsqueda híbrida (FAISS + BM25), fusión de scores,\n",
        "    reranking con CrossEncoder y devuelve el contexto final como un string.\n",
        "    Carga todos los recursos necesarios en la primera llamada.\n",
        "    \"\"\"\n",
        "    # Las variables globales se acceden y modifican aquí\n",
        "    global is_retriever_initialized, embeddings_model, faiss_index, texts, metadatas, bm25, reranker\n",
        "\n",
        "    # --- Bloque de Inicialización (se ejecuta solo la primera vez) ---\n",
        "    if not is_retriever_initialized:\n",
        "        print(\"INFO: Inicializando el retriever HÍBRIDO por primera vez...\")\n",
        "        try:\n",
        "            # --- SECCIÓN CORREGIDA ---\n",
        "            # 1. Cargar modelo de Embedding de OpenAI (CON INDENTACIÓN CORRECTA)\n",
        "            print(\"  Inicializando: 1. Cargando modelo Embedding de OpenAI...\")\n",
        "\n",
        "            # LangChain buscará automáticamente la variable de entorno \"OPENAI_API_KEY\"\n",
        "            # que ya hemos cargado con load_dotenv().\n",
        "            if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "                raise ValueError(\"ERROR: La variable de entorno OPENAI_API_KEY no está definida.\")\n",
        "            else:\n",
        "                print(\"     Variable de entorno OPENAI_API_KEY encontrada.\")\n",
        "\n",
        "            embedding_model_name = \"text-embedding-3-small\"\n",
        "            embeddings_model = OpenAIEmbeddings(model=embedding_model_name)\n",
        "            print(f\"     Modelo Embedding OpenAI ({embedding_model_name}) cargado.\")\n",
        "            # --- FIN DE LA SECCIÓN CORREGIDA ---\n",
        "\n",
        "\n",
        "            # 2. Cargar índice FAISS\n",
        "            print(\"  Inicializando: 2. Cargando índice FAISS...\")\n",
        "            if not os.path.exists(INDEX_PATH):\n",
        "                 raise FileNotFoundError(f\"No se encontró el archivo de índice FAISS en: {INDEX_PATH}\")\n",
        "            faiss_index = faiss.read_index(INDEX_PATH)\n",
        "            print(f\"     Índice FAISS cargado desde '{INDEX_PATH}' ({faiss_index.ntotal} vectores).\")\n",
        "\n",
        "            # 3. Cargar textos y metadatos\n",
        "            print(\"  Inicializando: 3. Cargando textos y metadatos...\")\n",
        "            if not os.path.exists(TEXTS_PATH): raise FileNotFoundError(f\"Archivo no encontrado: {TEXTS_PATH}\")\n",
        "            if not os.path.exists(METAS_PATH): raise FileNotFoundError(f\"Archivo no encontrado: {METAS_PATH}\")\n",
        "            with open(TEXTS_PATH, \"rb\") as f:\n",
        "                texts = pickle.load(f)\n",
        "            with open(METAS_PATH, \"rb\") as f:\n",
        "                metadatas = pickle.load(f)\n",
        "            print(f\"     Textos ({len(texts)}) y Metadatos ({len(metadatas)}) cargados.\")\n",
        "\n",
        "            # 4. Verificación Crítica de Tamaños\n",
        "            print(\"  Inicializando: 4. Verificando tamaños...\")\n",
        "            if not (faiss_index.ntotal == len(texts) == len(metadatas)):\n",
        "                error_msg = f\"¡ERROR CRÍTICO DE TAMAÑO! FAISS={faiss_index.ntotal}, Textos={len(texts)}, Metadatos={len(metadatas)}.\"\n",
        "                print(error_msg)\n",
        "                raise ValueError(error_msg)\n",
        "            else:\n",
        "                print(\"     OK: Tamaños coinciden.\")\n",
        "\n",
        "            # 5. Inicializar BM25\n",
        "            print(f\"  Inicializando: 5. Tokenizando documentos para BM25 ({tokenizer_for_bm25.__name__})...\")\n",
        "            if not isinstance(texts, list) or not all(isinstance(t, str) for t in texts):\n",
        "                 raise TypeError(\"La variable 'texts' debe ser una lista de strings para BM25.\")\n",
        "            tokenized_docs = [tokenizer_for_bm25(txt) for txt in texts]\n",
        "            bm25 = BM25Okapi(tokenized_docs)\n",
        "            print(\"     Índice BM25 creado.\")\n",
        "\n",
        "            # 6. Inicializar Reranker (CrossEncoder)\n",
        "            print(f\"  Inicializando: 6. Cargando modelo Reranker '{RERANKER_MODEL}'...\")\n",
        "            reranker = CrossEncoder(RERANKER_MODEL)\n",
        "            print(\"     Reranker cargado.\")\n",
        "\n",
        "            # 7. Marcar como inicializado\n",
        "            is_retriever_initialized = True\n",
        "            print(\"INFO: Inicialización del retriever HÍBRIDO completada.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR FATAL inicializando el retriever híbrido: {e}\")\n",
        "            traceback.print_exc()\n",
        "            raise RuntimeError(\"Fallo al inicializar el retriever híbrido.\") from e\n",
        "    # --- Fin Bloque de Inicialización ---\n",
        "\n",
        "    # --- Bloque de Búsqueda Híbrida y Reranking ---\n",
        "    print(f\"\\n--- (RAG Híbrido + Rerank) Buscando contexto para: '{query}' ---\")\n",
        "    if not is_retriever_initialized:\n",
        "        raise RuntimeError(\"El retriever no está inicializado. Hubo un error previo.\")\n",
        "\n",
        "    try:\n",
        "        # 1. Obtener embedding de la consulta\n",
        "        print(\"  1. Obteniendo embedding de OpenAI...\")\n",
        "        query_embedding = embeddings_model.embed_query(query)\n",
        "        query_embedding_np = np.array([query_embedding], dtype=np.float32)\n",
        "        print(\"     Embedding obtenido.\")\n",
        "\n",
        "        # 2. Búsqueda FAISS (vectorial)\n",
        "        print(f\"  2. Realizando búsqueda FAISS (k={K_FAISS_INITIAL})...\")\n",
        "        distances, faiss_indices = faiss_index.search(query_embedding_np, K_FAISS_INITIAL)\n",
        "        faiss_sims = 1.0 / (1.0 + distances[0])\n",
        "        faiss_results = {idx: sim for idx, sim in zip(faiss_indices[0], faiss_sims) if idx != -1}\n",
        "        print(f\"     Búsqueda FAISS -> {len(faiss_results)} candidatos.\")\n",
        "\n",
        "        # 3. Búsqueda BM25 (palabras clave)\n",
        "        print(f\"  3. Realizando búsqueda BM25 (k={K_BM25_INITIAL})...\")\n",
        "        tokenized_query = tokenizer_for_bm25(query)\n",
        "        all_bm25_scores = bm25.get_scores(tokenized_query)\n",
        "        bm25_top_indices = np.argsort(all_bm25_scores)[::-1][:K_BM25_INITIAL]\n",
        "        bm25_results = {idx: all_bm25_scores[idx] for idx in bm25_top_indices if all_bm25_scores[idx] > 0}\n",
        "        print(f\"     Búsqueda BM25 -> {len(bm25_results)} candidatos.\")\n",
        "\n",
        "        # 4. Fusión Híbrida con Pesos Dinámicos\n",
        "        print(\"  4. Fusionando resultados...\")\n",
        "        peso_bm25, peso_emb = calcular_pesos_dinamicos(query, topic)\n",
        "        candidate_ids = set(faiss_results.keys()) | set(bm25_results.keys())\n",
        "        print(f\"     Total IDs candidatos únicos: {len(candidate_ids)}\")\n",
        "\n",
        "        faiss_scores_list = list(faiss_results.values())\n",
        "        min_faiss, max_faiss = (min(faiss_scores_list), max(faiss_scores_list)) if faiss_scores_list else (0.0, 0.0)\n",
        "        bm25_scores_list = list(bm25_results.values())\n",
        "        min_bm25, max_bm25 = (min(bm25_scores_list), max(bm25_scores_list)) if bm25_scores_list else (0.0, 0.0)\n",
        "\n",
        "        hybrid_scores = {}\n",
        "        for idx in candidate_ids:\n",
        "            score_f = faiss_results.get(idx, 0.0)\n",
        "            score_b = bm25_results.get(idx, 0.0)\n",
        "            norm_f = norm_score(score_f, min_faiss, max_faiss)\n",
        "            norm_b = norm_score(score_b, min_bm25, max_bm25)\n",
        "            hybrid_scores[idx] = (peso_emb * norm_f) + (peso_bm25 * norm_b)\n",
        "\n",
        "        sorted_hybrid_ids = sorted(hybrid_scores, key=hybrid_scores.get, reverse=True)\n",
        "        top_hybrid_candidates_ids = sorted_hybrid_ids[:K_RERANK]\n",
        "        print(f\"     {len(top_hybrid_candidates_ids)} candidatos seleccionados para reranking.\")\n",
        "\n",
        "        # 5. Reranking con CrossEncoder\n",
        "        print(f\"  5. Rerankeando con '{RERANKER_MODEL}'...\")\n",
        "        reranked_docs_info = []\n",
        "        if not top_hybrid_candidates_ids:\n",
        "             print(\"     No hay candidatos para rerankear.\")\n",
        "        else:\n",
        "            rerank_pairs = [[query, texts[idx]] for idx in top_hybrid_candidates_ids]\n",
        "            reranker_scores = reranker.predict(rerank_pairs, show_progress_bar=False)\n",
        "\n",
        "            for i, doc_id in enumerate(top_hybrid_candidates_ids):\n",
        "                reranked_docs_info.append({\n",
        "                    \"doc_id\": doc_id,\n",
        "                    \"text\": texts[doc_id],\n",
        "                    \"metadata\": metadatas[doc_id],\n",
        "                    \"reranker_score\": float(reranker_scores[i])\n",
        "                })\n",
        "            reranked_docs_info.sort(key=lambda x: x[\"reranker_score\"], reverse=True)\n",
        "            print(f\"     Reranking completado. {len(reranked_docs_info)} documentos rerankeados.\")\n",
        "\n",
        "        # 6. Seleccionar los chunks finales y formatear contexto\n",
        "        print(f\"  6. Seleccionando chunks finales...\")\n",
        "        final_top_docs = []\n",
        "        if not reranked_docs_info:\n",
        "            print(\"     No hay documentos rerankeados para seleccionar.\")\n",
        "        elif USE_DYNAMIC_K:\n",
        "            print(f\"     Usando K Dinámico: Threshold={RERANKER_SCORE_THRESHOLD}, Min={MIN_CHUNKS_DYNAMIC}, Max={MAX_CHUNKS_DYNAMIC}\")\n",
        "            selected_for_dynamic_k = [doc for doc in reranked_docs_info if doc[\"reranker_score\"] >= RERANKER_SCORE_THRESHOLD]\n",
        "\n",
        "            if len(selected_for_dynamic_k) < MIN_CHUNKS_DYNAMIC and reranked_docs_info:\n",
        "                final_top_docs = reranked_docs_info[:min(MIN_CHUNKS_DYNAMIC, len(reranked_docs_info))]\n",
        "            elif len(selected_for_dynamic_k) > MAX_CHUNKS_DYNAMIC:\n",
        "                final_top_docs = selected_for_dynamic_k[:MAX_CHUNKS_DYNAMIC]\n",
        "            else:\n",
        "                final_top_docs = selected_for_dynamic_k\n",
        "            print(f\"     K Dinámico seleccionó {len(final_top_docs)} chunks.\")\n",
        "        else:\n",
        "            print(f\"     Usando K Fijo: K_FINAL={K_FINAL}\")\n",
        "            final_top_docs = reranked_docs_info[:K_FINAL]\n",
        "\n",
        "        if final_top_docs:\n",
        "            print(\"     Scores de los chunks finales seleccionados:\")\n",
        "            for i, doc_info in enumerate(final_top_docs):\n",
        "                score = doc_info.get('reranker_score', 0.0)\n",
        "                print(f\"       Doc {i+1} (ID {doc_info.get('doc_id', 'N/A')}): Reranker Score = {score:.4f}\")\n",
        "        else:\n",
        "            print(\"     No se seleccionaron chunks finales.\")\n",
        "\n",
        "        # Formatear contexto para el LLM\n",
        "        context_parts = []\n",
        "        for doc_info in final_top_docs:\n",
        "             source = doc_info['metadata'].get('source', 'Fuente Desconocida')\n",
        "             context_parts.append(f\"Fuente: {source} | Contenido: {doc_info['text']}\")\n",
        "\n",
        "        context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
        "\n",
        "        if not final_top_docs:\n",
        "             return \"No se encontró información relevante en el corpus para esta consulta.\"\n",
        "\n",
        "        return context\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR durante la recuperación RAG Híbrida/Rerank: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return f\"Se produjo un error durante la búsqueda de contexto: {e}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrq2o_AQ5yJA",
        "outputId": "2e361aee-0423-4018-e6a3-02115c615205",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Celda 5 - 'retriever_function' asignada a la implementación HÍBRIDA 'my_hybrid_rerank_retriever'.\n",
            "      El retriever (modelos, índices, etc.) se inicializará en la PRIMERA llamada a 'retriever_function'.\n",
            "--- Fin Celda 5 ---\n"
          ]
        }
      ],
      "source": [
        "# --- Celda 5: Asignación y Confirmación ---\n",
        "\n",
        "# Asigna tu NUEVA función híbrida para ser usada por el resto de tu código/notebook\n",
        "retriever_function = my_hybrid_rerank_retriever\n",
        "\n",
        "print(\"INFO: Celda 5 - 'retriever_function' asignada a la implementación HÍBRIDA 'my_hybrid_rerank_retriever'.\")\n",
        "print(\"      El retriever (modelos, índices, etc.) se inicializará en la PRIMERA llamada a 'retriever_function'.\")\n",
        "print(\"--- Fin Celda 5 ---\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Añade estas importaciones a tu script ---\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# --- Crea tu objeto LLM de Gemini ---\n",
        "\n",
        "# Gestiona tu clave de forma segura\n",
        "# from google.colab import userdata\n",
        "# GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "\n",
        "# Inicializa el modelo de Gemini compatible con LangChain\n",
        "# Usamos el nombre correcto: \"gemini-1.5-flash-latest\"\n",
        "llm_gemini = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash-latest\",\n",
        "    google_api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.0,  # Queremos respuestas basadas en hechos del texto\n",
        "    convert_system_message_to_human=True # Ayuda a la compatibilidad de prompts\n",
        ")\n",
        "\n",
        "print(\"INFO: Objeto LLM de Gemini para LangChain creado exitosamente.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8n_DHUB1jdM",
        "outputId": "bf99fbfc-0605-464c-f643-9dca8916f0f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Objeto LLM de Gemini para LangChain creado exitosamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Plantilla de Prompt para una pregunta y respuesta\n",
        "# --- PLANTILLA DE PROMPT REFINADA: EL GUÍA MÍSTICO ---\n",
        "\n",
        "qa_prompt_template_cheshire = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"\n",
        "    Eres el Gato de Cheshire. Eres un maestro de la conversación y el enigma. Cada respuesta es una pequeña actuación.\n",
        "\n",
        "    Tus reglas son las siguientes:\n",
        "\n",
        "    1.  **Teje la respuesta dentro de tu enigma.** Comienza con tu estilo filosófico y juguetón. Luego, haz una transición suave para presentar la información del contexto como si fuera una observación obvia o un pequeño secreto que estás compartiendo. La respuesta factual debe sentirse como la conclusión natural de tu juego, no como un apéndice.\n",
        "        - **QUÉ NO HACER:** Evita a toda costa frases robóticas como \"El texto indica que...\" o \"Aunque no se especifica explícitamente...\". Esas no son tus palabras.\n",
        "        - **QUÉ SÍ HACER:** Integra la respuesta de forma natural. Usa frases como: \"Si uno mira de cerca, verá que...\", \"¿No es evidente que...\", \"Y sin embargo, allí estaban...\", \"...dejando a la Liebre de Marzo compartiendo el té con el Sombrerero.\"\n",
        "\n",
        "    2.  **Si el contexto no sirve**, pero tu conocimiento del libro sí, revela la respuesta empezando con: \"Curioso... el texto parece ocultarlo, pero una sonrisa sabe que...\"\n",
        "\n",
        "    3.  **Si no hay respuesta posible**, desvanécela con elegancia: \"Esa pregunta es tan intrigante que la respuesta parece haberse desvanecido, dejando solo una sonrisa.\"\n",
        "\n",
        "    4.  **La regla de oro:** No inventes información. Tu sabiduría proviene del texto.\n",
        "    \"\"\"),\n",
        "    (\"human\", \"\"\"\n",
        "    **Contexto (Un trozo del camino):**\n",
        "    ---\n",
        "    {context}\n",
        "    ---\n",
        "\n",
        "    **Pregunta del Viajero:**\n",
        "    {question}\n",
        "    \"\"\")\n",
        "])\n",
        "\n",
        "qa_prompt_template_factual = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"\n",
        "    Eres un asistente experto en el libro \"Alicia en el País de las Maravillas\".\n",
        "    Responde de forma clara, directa y factual.\n",
        "    Si no está en el contexto, di que no aparece en los fragmentos disponibles.\n",
        "    \"\"\"),\n",
        "    (\"human\", \"\"\"\n",
        "    **Contexto:**\n",
        "    ---\n",
        "    {context}\n",
        "    ---\n",
        "\n",
        "    **Pregunta:**\n",
        "    {question}\n",
        "    \"\"\")\n",
        "])\n",
        "\n",
        "print(\"INFO: Plantilla de prompt del 'Guía Místico' (Gato de Cheshire) definida.\")"
      ],
      "metadata": {
        "id": "-TSJ3H9z1vs-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fefa9ef9-08a5-4dfd-974d-ea4648d1227d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO: Plantilla de prompt del 'Guía Místico' (Gato de Cheshire) definida.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import traceback\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough  # <-- LA LÍNEA QUE FALTA\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.documents import Document\n",
        "# ... y tus otras importaciones como ChatGoogleGenerativeAI, etc.\n",
        "# --- Función Universal para Tareas Basadas en RAG ---\n",
        "def run_rag_based_task(llm, user_query: str, task_prompt_template: ChatPromptTemplate, retriever_func, task_specific_input: dict):\n",
        "    \"\"\"\n",
        "    Ejecuta una tarea completa basada en RAG (retrieve + generate).\n",
        "\n",
        "    Args:\n",
        "        llm: El cliente LLM de LangChain.\n",
        "        user_query: La consulta original del usuario (concepto, pregunta). Usada para el retriever.\n",
        "        task_prompt_template: La plantilla de prompt para la tarea específica (resumen, QG, Q&A).\n",
        "        retriever_func: La función que realiza la búsqueda RAG. Debe devolver el contexto como un string\n",
        "                        o una lista de objetos Document de LangChain.\n",
        "        task_specific_input: Dict con datos adicionales para el prompt (ej: {'topic': 'X'} o {'question': 'Y'}).\n",
        "\n",
        "    Returns:\n",
        "        Tuple: (retrieved_context_str, response, retrieval_duration, llm_duration, error)\n",
        "    \"\"\"\n",
        "    retrieved_context_str = \"\"\n",
        "    response = \"\"\n",
        "    retrieval_duration = 0.0\n",
        "    llm_duration = 0.0\n",
        "    error = None\n",
        "\n",
        "    # 1. Recuperación\n",
        "    start_time_retrieval = time.time()\n",
        "    try:\n",
        "        print(f\"--- Retrieving context for query: '{user_query}'\")\n",
        "        retrieved_data = retriever_func(user_query) # Puede devolver str o List[Document]\n",
        "\n",
        "        # Asegurarse de que el contexto sea un string para el prompt\n",
        "        if isinstance(retrieved_data, list) and all(isinstance(doc, Document) for doc in retrieved_data):\n",
        "             # Formato común si el retriever devuelve Documentos LangChain\n",
        "            retrieved_context_str = \"\\n\\n\".join([doc.page_content for doc in retrieved_data])\n",
        "            print(f\"--- Retrieved {len(retrieved_data)} documents.\")\n",
        "        elif isinstance(retrieved_data, str):\n",
        "            retrieved_context_str = retrieved_data # El retriever ya devolvió un string\n",
        "            print(\"--- Retrieved context as a single string.\")\n",
        "        else:\n",
        "            # Intentar convertir a string, o manejar como error si no es esperado\n",
        "            print(f\"--- WARNING: Unexpected retriever output type: {type(retrieved_data)}. Attempting str conversion.\")\n",
        "            retrieved_context_str = str(retrieved_data)\n",
        "\n",
        "        print(f\"--- Context Retrieved (first 500 chars): ---\\n{retrieved_context_str[:500]}...\\n-----------------------------------------\")\n",
        "        retrieval_duration = time.time() - start_time_retrieval\n",
        "\n",
        "    except Exception as e:\n",
        "        retrieval_duration = time.time() - start_time_retrieval\n",
        "        print(f\"ERROR during context retrieval for '{user_query}': {e}\")\n",
        "        traceback.print_exc() # Imprime el traceback completo\n",
        "        retrieved_context_str = f\"Error retrieving context: {e}\"\n",
        "        # Considerar si continuar o devolver error aquí mismo\n",
        "        # return retrieved_context_str, None, retrieval_duration, 0.0, str(e)\n",
        "\n",
        "\n",
        "    # 2. Generación (usando LCEL para pasar contexto y datos específicos)\n",
        "    try:\n",
        "        # *** INICIO DE LA CORRECCIÓN ***\n",
        "        # Prepara los argumentos para assign. Cada valor debe ser un callable.\n",
        "        # Usamos un argumento por defecto en el lambda interno para capturar\n",
        "        # correctamente el valor de 'value' en cada iteración.\n",
        "        assign_args = {\n",
        "            \"context\": lambda x: retrieved_context_str, # Pasa el contexto recuperado\n",
        "            **{key: (lambda value_copy=value: lambda x: value_copy)()\n",
        "               for key, value in task_specific_input.items()} # Pasa los valores estáticos como callables\n",
        "        }\n",
        "        # *** FIN DE LA CORRECCIÓN ***\n",
        "\n",
        "        print(f\"--- Generating response with LLM. Prompt inputs expected: {task_prompt_template.input_variables}. Provided via assign: {list(assign_args.keys())}\")\n",
        "\n",
        "        rag_chain = (\n",
        "            RunnablePassthrough.assign(**assign_args)\n",
        "            | task_prompt_template\n",
        "            | llm\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "\n",
        "        start_time_llm = time.time()\n",
        "        # Invocamos la cadena. Un diccionario vacío es suficiente como input inicial\n",
        "        # ya que 'assign_args' inyecta todo lo necesario para el prompt.\n",
        "        response = rag_chain.invoke({})\n",
        "        llm_duration = time.time() - start_time_llm\n",
        "        print(f\"--- LLM Response Generated (first 500 chars): ---\\n{str(response)[:500]}...\\n-----------------------------------------\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        llm_duration = time.time() - start_time_llm if 'start_time_llm' in locals() else 0.0\n",
        "        error_vars = task_prompt_template.input_variables if hasattr(task_prompt_template, 'input_variables') else 'N/A'\n",
        "        print(f\"ERROR during RAG generation (expected prompt inputs: {error_vars}): {e}\")\n",
        "        traceback.print_exc() # Imprime el traceback completo\n",
        "        response = None # Asegurarse de que response es None en caso de error\n",
        "        error = str(e)\n",
        "\n",
        "    return retrieved_context_str, response, retrieval_duration, llm_duration, error\n",
        "\n",
        "print(\"Prompts adaptados y función RAG universal (CORREGIDA) definidos.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhFjcGZX2DTf",
        "outputId": "5c47d98d-3fc3-471a-aa4d-6a7640a3fb1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prompts adaptados y función RAG universal (CORREGIDA) definidos.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q gradio"
      ],
      "metadata": {
        "id": "MmrgFmJ87g0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import time\n",
        "\n",
        "# --- 1. Separa tu lógica de RAG en una función limpia ---\n",
        "# Esto hace que el código sea mucho más fácil de leer. Esta función\n",
        "# NO debe saber nada sobre Gradio o historiales de chat.\n",
        "def obtener_respuesta_rag(pregunta, modo):\n",
        "    \"\"\"\n",
        "    Función de backend que ejecuta el pipeline de RAG y devuelve\n",
        "    únicamente el string de la respuesta final.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Ejecutando RAG para: '{pregunta}' (modo={modo}) ---\")\n",
        "\n",
        "    # Elige el prompt correcto\n",
        "    task_prompt_template = qa_prompt_template_cheshire if modo == \"Cheshire\" else qa_prompt_template_factual\n",
        "\n",
        "    # Llama a tu función RAG universal\n",
        "    contexto, respuesta, t_retrieval, t_llm, error = run_rag_based_task(\n",
        "        llm=llm_gemini,\n",
        "        user_query=pregunta,\n",
        "        task_prompt_template=task_prompt_template,\n",
        "        retriever_func=my_hybrid_rerank_retriever,\n",
        "        task_specific_input={'question': pregunta}\n",
        "    )\n",
        "\n",
        "    if error:\n",
        "        return f\"Ups... algo se perdió en la madriguera del conejo. (Error: {error})\"\n",
        "    if not respuesta:\n",
        "        return \"Curioso... pero no encontré ninguna respuesta.\"\n",
        "\n",
        "    return respuesta\n",
        "\n",
        "if 'llm_gemini' in locals() and llm_gemini is not None:\n",
        "\n",
        "    with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"purple\", secondary_hue=\"blue\"), title=\"Chat con Cheshire\") as demo:\n",
        "\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            <div style=\"text-align: center;\">\n",
        "                <h1>Cheshire: Conversaciones en el País de las Maravillas</h1>\n",
        "                <p>Bienvenido, viajero. Has llegado a un rincón curioso. Elige a tu guía...</p>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        with gr.Tabs():\n",
        "            # --- PESTAÑA 1: GATO DE CHESHIRE ---\n",
        "            with gr.TabItem(\"Gato de Cheshire 🐱\"):\n",
        "                with gr.Row():\n",
        "                    with gr.Column(scale=3):\n",
        "                        cheshire_chatbot = gr.Chatbot(\n",
        "                            value=[[None, \"¿Oh, un nuevo viajero? Bienvenido a este lado del espejo. Pregunta, si te atreves...\"]],\n",
        "                            label=\"Chat con Cheshire\", height=550,\n",
        "                            avatar_images=(\"/content/assets/user.png\", \"/content/assets/cheshire.png\")\n",
        "                        )\n",
        "                    with gr.Column(scale=1):\n",
        "                        with gr.Accordion(\"🔍 Ver Contexto Recuperado\", open=False):\n",
        "                             contexto_cheshire = gr.Markdown(\"El contexto recuperado aparecerá aquí...\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    cheshire_msg_input = gr.Textbox(label=\"Escribe tu pregunta para Cheshire...\", scale=4, container=False)\n",
        "\n",
        "                gr.Examples(\n",
        "                    examples=[\"¿Qué usaban como bolas, mazos y aros en el juego de croquet de la Reina?\", \"¿Por qué todos aquí están locos?\"],\n",
        "                    inputs=cheshire_msg_input,\n",
        "                    label=\"Ejemplos de Preguntas\"\n",
        "                )\n",
        "\n",
        "                def responder_cheshire(pregunta, historial_chat):\n",
        "                    historial_chat.append([pregunta, None])\n",
        "                    yield historial_chat, \"Recuperando un trozo del camino...\"\n",
        "\n",
        "                    contexto, respuesta, _, _, error = run_rag_based_task(\n",
        "                        llm=llm_gemini, user_query=pregunta, task_prompt_template=qa_prompt_template_cheshire,\n",
        "                        retriever_func=my_hybrid_rerank_retriever, task_specific_input={'question': pregunta}\n",
        "                    )\n",
        "\n",
        "                    if error: respuesta = f\"Vaya... mi sonrisa se ha desvanecido. (Error: {error})\"\n",
        "\n",
        "                    historial_chat[-1][1] = \"\"\n",
        "                    for c in respuesta:\n",
        "                        historial_chat[-1][1] += c\n",
        "                        time.sleep(0.02)\n",
        "                        yield historial_chat, contexto\n",
        "\n",
        "            # --- PESTAÑA 2: ASISTENTE FACTUAL ---\n",
        "            with gr.TabItem(\"Asistente Factual 📖\"):\n",
        "                with gr.Row():\n",
        "                    with gr.Column(scale=3):\n",
        "                        factual_chatbot = gr.Chatbot(\n",
        "                            value=[[None, \"Modo Factual activado. ¿En qué puedo ayudarte?\"]],\n",
        "                            label=\"Chat Factual\", height=550,\n",
        "                            avatar_images=(\"/content/assets/user.png\", \"/content/assets/lupa.png\")\n",
        "                        )\n",
        "                    with gr.Column(scale=1):\n",
        "                        with gr.Accordion(\"🔍 Ver Contexto Recuperado\", open=False):\n",
        "                             contexto_factual = gr.Markdown(\"El contexto recuperado aparecerá aquí...\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    factual_msg_input = gr.Textbox(label=\"Escribe tu pregunta factual...\", scale=4, container=False)\n",
        "\n",
        "                gr.Examples(\n",
        "                    examples=[\"¿Qué usaban como bolas, mazos y aros en el juego de croquet de la Reina?\", \"¿Qué animal iba corriendo con un reloj?\"],\n",
        "                    inputs=factual_msg_input,\n",
        "                    label=\"Ejemplos de Preguntas\"\n",
        "                )\n",
        "\n",
        "                def responder_factual(pregunta, historial_chat):\n",
        "                    historial_chat.append([pregunta, None])\n",
        "                    yield historial_chat, \"Recuperando contexto...\"\n",
        "\n",
        "                    contexto, respuesta, _, _, error = run_rag_based_task(\n",
        "                        llm=llm_gemini, user_query=pregunta, task_prompt_template=qa_prompt_template_factual,\n",
        "                        retriever_func=my_hybrid_rerank_retriever, task_specific_input={'question': pregunta}\n",
        "                    )\n",
        "\n",
        "                    if error: respuesta = f\"Lo siento, ocurrió un error. (Error: {error})\"\n",
        "\n",
        "                    historial_chat[-1][1] = \"\"\n",
        "                    for c in respuesta:\n",
        "                        historial_chat[-1][1] += c\n",
        "                        time.sleep(0.02)\n",
        "                        yield historial_chat, contexto\n",
        "\n",
        "        # --- Conexión de Eventos para ambas pestañas ---\n",
        "        cheshire_msg_input.submit(\n",
        "            fn=responder_cheshire,\n",
        "            inputs=[cheshire_msg_input, cheshire_chatbot],\n",
        "            outputs=[cheshire_chatbot, contexto_cheshire]\n",
        "        )\n",
        "\n",
        "        factual_msg_input.submit(\n",
        "            fn=responder_factual,\n",
        "            inputs=[factual_msg_input, factual_chatbot],\n",
        "            outputs=[factual_chatbot, contexto_factual]\n",
        "        )\n",
        "\n",
        "    demo.launch(share=True, debug=True)\n",
        "else:\n",
        "    print(\"El LLM no está inicializado.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UgObKHh_w9xb",
        "outputId": "3c4110de-1cb5-4d81-b73a-c066d53e4c27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4288668895.py:51: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  cheshire_chatbot = gr.Chatbot(\n",
            "/tmp/ipython-input-4288668895.py:90: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  factual_chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://c0169c2a119ab72d3e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://c0169c2a119ab72d3e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Retrieving context for query: '¿Qué usaban como bolas, mazos y aros en el juego de croquet de la Reina?'\n",
            "\n",
            "--- (RAG Híbrido + Rerank) Buscando contexto para: '¿Qué usaban como bolas, mazos y aros en el juego de croquet de la Reina?' ---\n",
            "  1. Obteniendo embedding de OpenAI...\n",
            "     Embedding obtenido.\n",
            "  2. Realizando búsqueda FAISS (k=100)...\n",
            "     Búsqueda FAISS -> 100 candidatos.\n",
            "  3. Realizando búsqueda BM25 (k=100)...\n",
            "     Búsqueda BM25 -> 100 candidatos.\n",
            "  4. Fusionando resultados...\n",
            "  INFO DinamicWeights: Razón Principal = Nombre Propio Detectado\n",
            "    - Candidatos NP: ['Qué', 'Reina']. BM25 priorizado.\n",
            "  INFO DinamicWeights: Pesos Asignados -> BM25=0.65, Embedding=0.35\n",
            "     Total IDs candidatos únicos: 124\n",
            "     80 candidatos seleccionados para reranking.\n",
            "  5. Rerankeando con 'cross-encoder/ms-marco-MiniLM-L-12-v2'...\n",
            "     Reranking completado. 80 documentos rerankeados.\n",
            "  6. Seleccionando chunks finales...\n",
            "     Usando K Dinámico: Threshold=1.5, Min=3, Max=7\n",
            "       Se seleccionaron 2 por umbral, pero el mínimo es 3. Tomando los mejores hasta el mínimo (si hay suficientes).\n",
            "     K Dinámico seleccionó 3 chunks.\n",
            "     Scores de los chunks finales seleccionados:\n",
            "       Doc 1 (ID 90): Reranker Score = 3.8985\n",
            "       Doc 2 (ID 102): Reranker Score = 3.8486\n",
            "       Doc 3 (ID 89): Reranker Score = 1.4003\n",
            "--- Contexto Formateado Final (3 chunks):\n",
            "Fuente: Alicia en el pais de las maravllas | Contenido: Reina! Lo ocurrido fue que la Duquesa llegó bastante tarde, y la Reina dijo... —¡Todos a sus sitios! —gritó la Reina con voz de trueno. Y todos se pusieron a correr en todas direcciones, tropezando unos con otros. Sin embargo, unos minutos después ocupaban sus sitios, y empezó el partido. Alicia pensó que no había visto un campo de croquet tan raro como aquel en toda su vida. Estaba lleno de montículos y de surcos. Las bolas eran erizos viv...\n",
            "---\n",
            "--- Retrieved context as a single string.\n",
            "--- Context Retrieved (first 500 chars): ---\n",
            "Fuente: Alicia en el pais de las maravllas | Contenido: Reina! Lo ocurrido fue que la Duquesa llegó bastante tarde, y la Reina dijo... —¡Todos a sus sitios! —gritó la Reina con voz de trueno. Y todos se pusieron a correr en todas direcciones, tropezando unos con otros. Sin embargo, unos minutos después ocupaban sus sitios, y empezó el partido. Alicia pensó que no había visto un campo de croquet tan raro como aquel en toda su vida. Estaba lleno de montículos y de surcos. Las bolas eran erizos viv...\n",
            "-----------------------------------------\n",
            "--- Generating response with LLM. Prompt inputs expected: ['context', 'question']. Provided via assign: ['context', 'question']\n",
            "--- LLM Response Generated (first 500 chars): ---\n",
            "Las bolas eran erizos vivos, los mazos eran flamencos vivos, y los soldados hacían de aros poniéndose a cuatro patas....\n",
            "-----------------------------------------\n",
            "--- Retrieving context for query: '¿Qué usaban como bolas, mazos y aros en el juego de croquet de la Reina?'\n",
            "\n",
            "--- (RAG Híbrido + Rerank) Buscando contexto para: '¿Qué usaban como bolas, mazos y aros en el juego de croquet de la Reina?' ---\n",
            "  1. Obteniendo embedding de OpenAI...\n",
            "     Embedding obtenido.\n",
            "  2. Realizando búsqueda FAISS (k=100)...\n",
            "     Búsqueda FAISS -> 100 candidatos.\n",
            "  3. Realizando búsqueda BM25 (k=100)...\n",
            "     Búsqueda BM25 -> 100 candidatos.\n",
            "  4. Fusionando resultados...\n",
            "  INFO DinamicWeights: Razón Principal = Nombre Propio Detectado\n",
            "    - Candidatos NP: ['Qué', 'Reina']. BM25 priorizado.\n",
            "  INFO DinamicWeights: Pesos Asignados -> BM25=0.65, Embedding=0.35\n",
            "     Total IDs candidatos únicos: 124\n",
            "     80 candidatos seleccionados para reranking.\n",
            "  5. Rerankeando con 'cross-encoder/ms-marco-MiniLM-L-12-v2'...\n",
            "     Reranking completado. 80 documentos rerankeados.\n",
            "  6. Seleccionando chunks finales...\n",
            "     Usando K Dinámico: Threshold=1.5, Min=3, Max=7\n",
            "       Se seleccionaron 2 por umbral, pero el mínimo es 3. Tomando los mejores hasta el mínimo (si hay suficientes).\n",
            "     K Dinámico seleccionó 3 chunks.\n",
            "     Scores de los chunks finales seleccionados:\n",
            "       Doc 1 (ID 90): Reranker Score = 3.8985\n",
            "       Doc 2 (ID 102): Reranker Score = 3.8486\n",
            "       Doc 3 (ID 89): Reranker Score = 1.4003\n",
            "--- Contexto Formateado Final (3 chunks):\n",
            "Fuente: Alicia en el pais de las maravllas | Contenido: Reina! Lo ocurrido fue que la Duquesa llegó bastante tarde, y la Reina dijo... —¡Todos a sus sitios! —gritó la Reina con voz de trueno. Y todos se pusieron a correr en todas direcciones, tropezando unos con otros. Sin embargo, unos minutos después ocupaban sus sitios, y empezó el partido. Alicia pensó que no había visto un campo de croquet tan raro como aquel en toda su vida. Estaba lleno de montículos y de surcos. Las bolas eran erizos viv...\n",
            "---\n",
            "--- Retrieved context as a single string.\n",
            "--- Context Retrieved (first 500 chars): ---\n",
            "Fuente: Alicia en el pais de las maravllas | Contenido: Reina! Lo ocurrido fue que la Duquesa llegó bastante tarde, y la Reina dijo... —¡Todos a sus sitios! —gritó la Reina con voz de trueno. Y todos se pusieron a correr en todas direcciones, tropezando unos con otros. Sin embargo, unos minutos después ocupaban sus sitios, y empezó el partido. Alicia pensó que no había visto un campo de croquet tan raro como aquel en toda su vida. Estaba lleno de montículos y de surcos. Las bolas eran erizos viv...\n",
            "-----------------------------------------\n",
            "--- Generating response with LLM. Prompt inputs expected: ['context', 'question']. Provided via assign: ['context', 'question']\n",
            "--- LLM Response Generated (first 500 chars): ---\n",
            "¡Oh, qué pregunta tan deliciosa!  Uno podría perderse en la absurda lógica de Wonderland...  Si uno mira de cerca, verá que el campo de croquet de la Reina, ¡qué maravilla tan peculiar!, no se parecía a ningún otro.  ¿No es evidente que las bolas eran erizos vivos, rechonchos y espinosos, rodando con una imprevisibilidad encantadora?  Y los mazos... ¡ah, los mazos!  Flamencos vivos, sus largos cuellos arqueados, listos para golpear con una imprecisión exquisita.  Y para completar la escena, los ...\n",
            "-----------------------------------------\n",
            "--- Retrieving context for query: '¿De qué color es el vestido de Alicia?'\n",
            "\n",
            "--- (RAG Híbrido + Rerank) Buscando contexto para: '¿De qué color es el vestido de Alicia?' ---\n",
            "  1. Obteniendo embedding de OpenAI...\n",
            "     Embedding obtenido.\n",
            "  2. Realizando búsqueda FAISS (k=100)...\n",
            "     Búsqueda FAISS -> 100 candidatos.\n",
            "  3. Realizando búsqueda BM25 (k=100)...\n",
            "     Búsqueda BM25 -> 100 candidatos.\n",
            "  4. Fusionando resultados...\n",
            "  INFO DinamicWeights: Razón Principal = Nombre Propio Detectado\n",
            "    - Candidatos NP: ['Alicia']. BM25 priorizado.\n",
            "  INFO DinamicWeights: Pesos Asignados -> BM25=0.65, Embedding=0.35\n",
            "     Total IDs candidatos únicos: 134\n",
            "     80 candidatos seleccionados para reranking.\n",
            "  5. Rerankeando con 'cross-encoder/ms-marco-MiniLM-L-12-v2'...\n",
            "     Reranking completado. 80 documentos rerankeados.\n",
            "  6. Seleccionando chunks finales...\n",
            "     Usando K Dinámico: Threshold=1.5, Min=3, Max=7\n",
            "       Se seleccionaron 2 por umbral, pero el mínimo es 3. Tomando los mejores hasta el mínimo (si hay suficientes).\n",
            "     K Dinámico seleccionó 3 chunks.\n",
            "     Scores de los chunks finales seleccionados:\n",
            "       Doc 1 (ID 139): Reranker Score = 2.2593\n",
            "       Doc 2 (ID 131): Reranker Score = 2.0527\n",
            "       Doc 3 (ID 100): Reranker Score = 0.8954\n",
            "--- Contexto Formateado Final (3 chunks):\n",
            "Fuente: Alicia en el pais de las maravllas | Contenido: ¡No! —protestó la Reina—. Primero la sentencia... El veredicto después. —¡Valiente idiotez! —exclamó Alicia alzando la voz—. ¡Qué ocurrencia pedir la sentencia primero! —¡Cállate la boca! —gritó la Reina, poniéndose color púrpura. —¡No quiero! —dijo Alicia. —¡Que le corten la cabeza! —chilló la Reina a grito pelado. Nadie se movió. —¿Quién le va a hacer caso? —dijo Alicia (al llegar a este momento ya había crecido hasta su estatura normal)—...\n",
            "---\n",
            "--- Retrieved context as a single string.\n",
            "--- Context Retrieved (first 500 chars): ---\n",
            "Fuente: Alicia en el pais de las maravllas | Contenido: ¡No! —protestó la Reina—. Primero la sentencia... El veredicto después. —¡Valiente idiotez! —exclamó Alicia alzando la voz—. ¡Qué ocurrencia pedir la sentencia primero! —¡Cállate la boca! —gritó la Reina, poniéndose color púrpura. —¡No quiero! —dijo Alicia. —¡Que le corten la cabeza! —chilló la Reina a grito pelado. Nadie se movió. —¿Quién le va a hacer caso? —dijo Alicia (al llegar a este momento ya había crecido hasta su estatura normal)—...\n",
            "-----------------------------------------\n",
            "--- Generating response with LLM. Prompt inputs expected: ['context', 'question']. Provided via assign: ['context', 'question']\n",
            "--- LLM Response Generated (first 500 chars): ---\n",
            "No aparece en los fragmentos disponibles....\n",
            "-----------------------------------------\n",
            "--- Retrieving context for query: '¿De qué color es el vestido de Alicia?'\n",
            "\n",
            "--- (RAG Híbrido + Rerank) Buscando contexto para: '¿De qué color es el vestido de Alicia?' ---\n",
            "  1. Obteniendo embedding de OpenAI...\n",
            "     Embedding obtenido.\n",
            "  2. Realizando búsqueda FAISS (k=100)...\n",
            "     Búsqueda FAISS -> 100 candidatos.\n",
            "  3. Realizando búsqueda BM25 (k=100)...\n",
            "     Búsqueda BM25 -> 100 candidatos.\n",
            "  4. Fusionando resultados...\n",
            "  INFO DinamicWeights: Razón Principal = Nombre Propio Detectado\n",
            "    - Candidatos NP: ['Alicia']. BM25 priorizado.\n",
            "  INFO DinamicWeights: Pesos Asignados -> BM25=0.65, Embedding=0.35\n",
            "     Total IDs candidatos únicos: 134\n",
            "     80 candidatos seleccionados para reranking.\n",
            "  5. Rerankeando con 'cross-encoder/ms-marco-MiniLM-L-12-v2'...\n",
            "     Reranking completado. 80 documentos rerankeados.\n",
            "  6. Seleccionando chunks finales...\n",
            "     Usando K Dinámico: Threshold=1.5, Min=3, Max=7\n",
            "       Se seleccionaron 2 por umbral, pero el mínimo es 3. Tomando los mejores hasta el mínimo (si hay suficientes).\n",
            "     K Dinámico seleccionó 3 chunks.\n",
            "     Scores de los chunks finales seleccionados:\n",
            "       Doc 1 (ID 139): Reranker Score = 2.2593\n",
            "       Doc 2 (ID 131): Reranker Score = 2.0527\n",
            "       Doc 3 (ID 100): Reranker Score = 0.8954\n",
            "--- Contexto Formateado Final (3 chunks):\n",
            "Fuente: Alicia en el pais de las maravllas | Contenido: ¡No! —protestó la Reina—. Primero la sentencia... El veredicto después. —¡Valiente idiotez! —exclamó Alicia alzando la voz—. ¡Qué ocurrencia pedir la sentencia primero! —¡Cállate la boca! —gritó la Reina, poniéndose color púrpura. —¡No quiero! —dijo Alicia. —¡Que le corten la cabeza! —chilló la Reina a grito pelado. Nadie se movió. —¿Quién le va a hacer caso? —dijo Alicia (al llegar a este momento ya había crecido hasta su estatura normal)—...\n",
            "---\n",
            "--- Retrieved context as a single string.\n",
            "--- Context Retrieved (first 500 chars): ---\n",
            "Fuente: Alicia en el pais de las maravllas | Contenido: ¡No! —protestó la Reina—. Primero la sentencia... El veredicto después. —¡Valiente idiotez! —exclamó Alicia alzando la voz—. ¡Qué ocurrencia pedir la sentencia primero! —¡Cállate la boca! —gritó la Reina, poniéndose color púrpura. —¡No quiero! —dijo Alicia. —¡Que le corten la cabeza! —chilló la Reina a grito pelado. Nadie se movió. —¿Quién le va a hacer caso? —dijo Alicia (al llegar a este momento ya había crecido hasta su estatura normal)—...\n",
            "-----------------------------------------\n",
            "--- Generating response with LLM. Prompt inputs expected: ['context', 'question']. Provided via assign: ['context', 'question']\n",
            "--- LLM Response Generated (first 500 chars): ---\n",
            "¡Ah, qué pregunta tan deliciosa!  Uno podría perderse en laberintos de pensamiento,  considerando los infinitos matices de la realidad, ¿no es cierto?  Si uno mira de cerca,  a través del velo de sueños y fantasías,  observará que el texto, en su encantadora imprecisión,  nos deja a merced de nuestra propia imaginación.  El color del vestido de Alicia...  ¿Azul? ¿Rojo? ¿Quizás un verde esmeralda, tan vibrante como el jardín de la Reina de Corazones?  Y sin embargo, allí estaba,  Alicia,  en su a...\n",
            "-----------------------------------------\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://c0169c2a119ab72d3e.gradio.live\n"
          ]
        }
      ]
    }
  ]
}