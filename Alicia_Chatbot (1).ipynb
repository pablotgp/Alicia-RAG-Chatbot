{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yjz8eEhpZr4t"
      },
      "outputs": [],
      "source": [
        "%pip install typing fastapi pypdf2 pymupdf os base64 traceback re collections math nltk faiss-cpu python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDQVdwyEbDKs"
      },
      "outputs": [],
      "source": [
        "%pip uninstall -y fitz\n",
        "%pip install PyMuPDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZMG6XuxTsI3R"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/Alicia-RAG-Chatbot') # Ajusta esta ruta si es necesario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQqEMp-eUQUH"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"mainipynb\n",
        "\n",
        "Automatically generated by Colab.\n",
        "\n",
        "Original file is located at\n",
        "    https://colab.research.google.com/drive/1DNTN2NtuOrcD-eiQAl31Y937Wpt5   vqBJ\n",
        "\"\"\"\n",
        "\n",
        "# main.py\n",
        "from typing import Any\n",
        "from fastapi import FastAPI, UploadFile, File\n",
        "from fastapi.responses import JSONResponse\n",
        "import fitz  # PyMuPDF\n",
        "import tempfile\n",
        "import os\n",
        "import base64\n",
        "import traceback\n",
        "import re\n",
        "from collections import defaultdict\n",
        "import math\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZy9yCGNUQUH"
      },
      "source": [
        "# Utilidades y limpieza de texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1cUohQcrUQUI"
      },
      "outputs": [],
      "source": [
        "def clean_pdf_text_robust(text):\n",
        "    \"\"\"Limpia texto de PDF de forma MÁS robusta para RAG, atacando patrones específicos.\"\"\"\n",
        "    if not text: return \"\"\n",
        "    # --- PASOS DE LIMPIEZA GENERAL ---\n",
        "    ligatures = {'ﬁ': 'fi', 'ﬂ': 'fl', 'ﬀ': 'ff', 'ﬃ': 'ffi', 'ﬄ': 'ffl'}\n",
        "    for lig, repl in ligatures.items(): text = text.replace(lig, repl)\n",
        "    text = re.sub(r'(\\w)-\\s*\\n\\s*(\\w)', r'\\1\\2', text) # Unir palabras con guión\n",
        "    text = re.sub(r'(\\w)-\\s*\\n\\s*(\\w)', r'\\1\\2', text) # Segunda pasada\n",
        "    text = re.sub(r'^\\s*Página\\s+\\d+(\\s+de\\s+\\d+)?\\s*$', '', text, flags=re.MULTILINE | re.IGNORECASE) # Paginación\n",
        "    text = re.sub(r'\\b\\d+\\s*/\\s*\\d+\\b', '', text) # Paginación X / Y\n",
        "    text = re.sub(r'https?://[^\\s/$.?#].[^\\s]*', '', text, flags=re.IGNORECASE) # URLs http/https\n",
        "    text = re.sub(r'\\bwww\\.[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\\b(?!\\.)', '', text) # URLs www\n",
        "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text) # Emails\n",
        "\n",
        "    # --- REGLAS ESPECÍFICAS MEJORADAS ---\n",
        "    text = re.sub(r'https?://opo\\.cl/[a-zA-Z0-9]+', '', text, flags=re.IGNORECASE) # URLs opo.cl\n",
        "    text = re.sub(r'\\bopositatest\\.com\\b', '', text, flags=re.IGNORECASE) # Dominio específico\n",
        "    text = re.sub(r'\\bv\\d+\\.\\d+\\.\\d+\\b', '', text, flags=re.IGNORECASE) # Versión vX.Y.Z\n",
        "    text = re.sub(r'/?\\s*\\+34\\s*(\\d{1,3}\\s*){2,4}', '', text) # Teléfono +34\n",
        "    text = re.sub(r'^\\s*\\d+\\s+TEMARIO\\s*$', '', text, flags=re.MULTILINE | re.IGNORECASE)\n",
        "    text = re.sub(r'^\\s*Accede a los recursos.*$', '', text, flags=re.MULTILINE | re.IGNORECASE) # Línea recursos\n",
        "    text = re.sub(r'^\\s*Comprueba si tu temario.*$', '', text, flags=re.MULTILINE | re.IGNORECASE) # Línea actualizado\n",
        "    text = re.sub(r'^\\s*ORGANIZACIÓN DEL ESTADO\\s*\\|\\s*TEMA\\s*\\d+\\s*$', '', text, flags=re.MULTILINE | re.IGNORECASE) # Cabecera específica\n",
        "    text = re.sub(r'^\\s*RECURSOS\\s*\\n?\\s*(GRÁFICOS)?\\s*$', '', text, flags=re.MULTILINE | re.IGNORECASE) # Cabecera Recursos\n",
        "    text = re.sub(r'^\\s*\\d+\\s*$', '', text, flags=re.MULTILINE) # Líneas solo con número (experimental)\n",
        "    # Eliminar bloque explicativo iconos (más agresivo)\n",
        "    text = re.sub(r'^\\s*RECURSOS\\s+GRÁFICOS.*?simple vistazo\\.', '', text, flags=re.IGNORECASE | re.DOTALL | re.MULTILINE)\n",
        "    text = re.sub(r'^\\s*PLAZOS\\s+Sabemos que.*?simple vistazo\\.', '', text, flags=re.IGNORECASE | re.DOTALL | re.MULTILINE)\n",
        "    text = re.sub(r'^\\s*(PLAZOS|Destacados|Pregunta de examen|Datos importantes|Negrita)\\s*$', '', text, flags=re.MULTILINE | re.IGNORECASE) # Títulos sueltos iconos\n",
        "\n",
        "    # --- PASOS DE NORMALIZACIÓN FINAL ---\n",
        "    text = re.sub(r'[ \\t\\f\\v]+', ' ', text) # Normalizar espacios horizontales\n",
        "    text = re.sub(r' +\\n', '\\n', text) # Espacios antes de salto\n",
        "    text = re.sub(r'\\n +', '\\n', text) # Espacios después de salto\n",
        "    text = re.sub(r'\\n{3,}', '\\n\\n', text) # Reducir saltos múltiples a 2\n",
        "    text = re.sub(r'^\\s*\\n', '', text, flags=re.MULTILINE) # Eliminar líneas vacías residuales\n",
        "    text = re.sub(r'^\\s*[-•*o»·]\\s+', '- ', text, flags=re.MULTILINE) # Normalizar viñetas\n",
        "    text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', text) # Caracteres de control\n",
        "    text = text.strip() # Limpiar inicio/fin\n",
        "    if text: text = text.rstrip('\\n') + '\\n\\n' # Asegurar que termine con dos saltos\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jffM_I_NUQUJ"
      },
      "source": [
        "# Detección de portada y secciones matemáticas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8R0sBVHUQUJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "import re\n",
        "\n",
        "# --- DETECCIÓN PORTADA (MODIFICADA) ---\n",
        "def is_likely_cover(page_text, page_number, num_total_pages):\n",
        "    \"\"\"Heurística para detectar portadas.\"\"\"\n",
        "\n",
        "    # 1. Limpieza y conteo inicial\n",
        "    lines = [line for line in page_text.split('\\n') if line.strip()]\n",
        "    line_count = len(lines)\n",
        "    text_length = len(page_text.strip())\n",
        "\n",
        "    # 2. Regla heurística extendida (para las primeras 15 páginas)\n",
        "    # Se considera portada/página preliminar si está en las primeras 15 páginas\n",
        "    # y tiene muy poco contenido (pocas líneas o pocos caracteres).\n",
        "    if page_number < 15 and (line_count < 15 or text_length < 200):\n",
        "         return True\n",
        "\n",
        "    # 3. Segunda regla heurística (para las primeras páginas, buscando palabras clave)\n",
        "    # Esta regla es más estricta y busca contenido editorial específico.\n",
        "    if page_number < 5 and line_count < 25:\n",
        "        # He modificado el rango de búsqueda a 'page_number < 5' para concentrar\n",
        "        # la búsqueda de palabras clave en las páginas iniciales, aunque podrías mantenerlo en 'page_number < 2'.\n",
        "\n",
        "        # Patrón que busca palabras clave editoriales/legales\n",
        "        if re.search(r'\\b(temario|edición|editorial|reservados todos los derechos|oposici[oó]n|ISBN|Copyright)\\b', page_text, re.IGNORECASE):\n",
        "            return True\n",
        "\n",
        "    # 4. Resultado por defecto\n",
        "    return False\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1r2c2s7UQUJ"
      },
      "source": [
        "# Detección de bibliografía e imágenes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdj7YySQUQUJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "def detectar_paginas_resumen_biblio(pdf_path, max_paginas_finales_a_revisar=10):\n",
        "    \"\"\"\n",
        "    Detecta páginas que contienen RESUMEN o BIBLIOGRAFÍA por separado.\n",
        "    \"\"\"\n",
        "    paginas_resumen = []\n",
        "    paginas_biblio = []\n",
        "\n",
        "    resumen_keywords = ['RESUMEN', 'CONCLUSIÓ']\n",
        "    biblio_keywords = ['BIBLIOGRAFÍA', 'REFERENCIAS', 'WEBGRAFÍA']\n",
        "\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "        num_total_pages = len(doc)\n",
        "        start_page_index = max(0, num_total_pages - max_paginas_finales_a_revisar)\n",
        "\n",
        "        for page_num in range(start_page_index, num_total_pages):\n",
        "            page = doc.load_page(page_num)\n",
        "            text = page.get_text(\"text\").upper()\n",
        "\n",
        "            if not text or text.isspace():\n",
        "                continue\n",
        "\n",
        "            # Detectar resumen\n",
        "            if any(re.search(r'(?:^[ \\t]*|\\n[ \\t]*)' + kw + r'\\b', text) for kw in resumen_keywords):\n",
        "                paginas_resumen.append(page_num)\n",
        "\n",
        "            # Detectar bibliografía\n",
        "            if any(re.search(r'(?:^[ \\t]*|\\n[ \\t]*)' + kw + r'\\b', text) for kw in biblio_keywords):\n",
        "                paginas_biblio.append(page_num)\n",
        "\n",
        "        doc.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"WARN (detectar_resumen_biblio): Error procesando {pdf_path}: {e}\")\n",
        "\n",
        "    return paginas_resumen, paginas_biblio\n",
        "\n",
        "\n",
        "def detect_image_regions_on_page(\n",
        "    page: Any,\n",
        "    merge_close_distance: int = 5,\n",
        "    min_area: int = 1000,\n",
        "    detect_drawings: bool = False,\n",
        "    debug: bool = False\n",
        ") -> list:\n",
        "    \"\"\"\n",
        "    Detecta regiones probables de imágenes (y opcionalmente dibujos vectoriales)\n",
        "    en una página de PyMuPDF, retornando bounding boxes fusionadas y filtradas.\n",
        "\n",
        "    Args:\n",
        "        page (fitz.Page): Página de PyMuPDF sobre la que se detectan imágenes.\n",
        "        merge_close_distance (int): Distancia máxima (en puntos) para fusionar\n",
        "            bounding boxes que se solapan o están muy cerca.\n",
        "        min_area (int): Área mínima (en puntos^2) para no descartar regiones pequeñas.\n",
        "        detect_drawings (bool): Si True, intentará detectar regiones vectoriales\n",
        "            (get_drawings()) y tratarlas como imágenes.\n",
        "        debug (bool): Si True, muestra mensajes de debug.\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: Lista de regiones detectadas, cada una con:\n",
        "            {\n",
        "              \"bbox\": (x0, y0, x1, y1),\n",
        "              \"type\": \"image\" | \"drawing\"\n",
        "            }\n",
        "    \"\"\"\n",
        "    all_regions = []\n",
        "    try:\n",
        "        # --------------------------------------------------------\n",
        "        # 1. DETECCIÓN DE IMÁGENES BITMAP\n",
        "        # --------------------------------------------------------\n",
        "        images_info = page.get_images(full=True)\n",
        "        for img_info in images_info:\n",
        "            xref = img_info[0]\n",
        "            if xref == 0:\n",
        "                continue  # ignorar imágenes inline o inválidas\n",
        "            try:\n",
        "                # Obtener los rectángulos donde se dibuja esta imagen (puede haber varios)\n",
        "                img_rects = page.get_image_rects(xref)\n",
        "                for rect in img_rects:\n",
        "                    bbox = rect.irect  # (x0, y0, x1, y1) con coords enteras\n",
        "                    x0, y0, x1, y1 = bbox\n",
        "                    area = (x1 - x0) * (y1 - y0)\n",
        "                    if area >= min_area:\n",
        "                        all_regions.append({\"bbox\": bbox, \"type\": \"image\"})\n",
        "                    elif debug:\n",
        "                        print(f\"DEBUG: Descartando imagen muy pequeña bbox={bbox}, area={area}\")\n",
        "            except Exception as err_rects:\n",
        "                if debug:\n",
        "                    print(f\"DEBUG: No se pudo obtener rects de imagen xref={xref}: {err_rects}\")\n",
        "\n",
        "        # --------------------------------------------------------\n",
        "        # 2. DETECCIÓN DE \"DRAWINGS\" VECTORIALES (OPCIONAL)\n",
        "        # --------------------------------------------------------\n",
        "        if detect_drawings:\n",
        "            try:\n",
        "                drawings = page.get_drawings()\n",
        "                for d in drawings:\n",
        "                    # 'type' puede ser: 'l' (line), 're' (rectangle),\n",
        "                    # 'f' (fill?), 'cs' (curves?), etc.\n",
        "                    # Ajusta según tus necesidades de filtrado.\n",
        "                    # Aquí descartamos líneas simples:\n",
        "                    if d['type'] == 'l':\n",
        "                        continue\n",
        "                    bbox = d['rect'].irect\n",
        "                    x0, y0, x1, y1 = bbox\n",
        "                    area = (x1 - x0) * (y1 - y0)\n",
        "                    if area >= min_area:\n",
        "                        all_regions.append({\"bbox\": bbox, \"type\": \"drawing\"})\n",
        "                    elif debug:\n",
        "                        print(f\"DEBUG: Descartando dibujo pequeño bbox={bbox}, area={area}\")\n",
        "            except Exception as err_draw:\n",
        "                if debug:\n",
        "                    print(f\"DEBUG: Error detectando dibujos vectoriales: {err_draw}\")\n",
        "\n",
        "        # --------------------------------------------------------\n",
        "        # 3. FUSIÓN DE BBOXES CERCANOS O SOLAPADOS\n",
        "        # --------------------------------------------------------\n",
        "        merged_regions = _merge_bounding_boxes(all_regions, merge_close_distance, debug=debug)\n",
        "\n",
        "        if debug:\n",
        "            print(f\"DEBUG: detect_image_regions_on_page => {len(all_regions)} sin fusionar, {len(merged_regions)} tras fusión\")\n",
        "\n",
        "        return merged_regions\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"WARN: Error detectando imágenes/dibujos en página: {e}\")\n",
        "        return []\n",
        "\n",
        "def _merge_bounding_boxes(regions: list, close_dist: int, debug: bool = False) -> list:\n",
        "    \"\"\"\n",
        "    Funde bounding boxes que se solapan o están muy cerca, retornando una\n",
        "    nueva lista de regiones. Cada región es un dict con:\n",
        "      { \"bbox\": (x0, y0, x1, y1), \"type\": \"image\" / \"drawing\" }.\n",
        "\n",
        "    - close_dist: se considerará 'cerca' si la distancia entre 2 rects\n",
        "      es menor o igual a close_dist.\n",
        "    - Este método hace un loop iterativo hasta que no haya merges nuevos.\n",
        "\n",
        "    Return: lista de dicts con bboxes fusionadas.\n",
        "    \"\"\"\n",
        "\n",
        "    # Para fusionar rects, necesitamos una pequeña función de \"check solape\" y \"unión\"\n",
        "    def rects_are_close_or_overlap(r1, r2, threshold):\n",
        "        \"\"\"Retorna True si r1 y r2 se solapan o la distancia entre ellos es <= threshold.\"\"\"\n",
        "        (x0a, y0a, x1a, y1a) = r1\n",
        "        (x0b, y0b, x1b, y1b) = r2\n",
        "\n",
        "        # 1) Si se solapan en x e y (overlap check)\n",
        "        overlap_x = not (x1a < x0b or x1b < x0a)\n",
        "        overlap_y = not (y1a < y0b or y1b < y0a)\n",
        "        if overlap_x and overlap_y:\n",
        "            return True\n",
        "\n",
        "        # 2) Si no solapan, calculamos distancia mínima entre los rects\n",
        "        #    Si es <= threshold, consideramos \"cerca\".\n",
        "        dist = _min_dist_between_rects(r1, r2)\n",
        "        return dist <= threshold\n",
        "\n",
        "    def merge_rects(r1, r2):\n",
        "        \"\"\"Devuelve el bounding box que cubre ambos rects.\"\"\"\n",
        "        (x0a, y0a, x1a, y1a) = r1\n",
        "        (x0b, y0b, x1b, y1b) = r2\n",
        "        return (\n",
        "            min(x0a, x0b),\n",
        "            min(y0a, y0b),\n",
        "            max(x1a, x1b),\n",
        "            max(y1a, y1b)\n",
        "        )\n",
        "\n",
        "    changed = True\n",
        "    while changed:\n",
        "        changed = False\n",
        "        merged_list = []\n",
        "        skip_indices = set()\n",
        "        n = len(regions)\n",
        "\n",
        "        for i in range(n):\n",
        "            if i in skip_indices:\n",
        "                continue\n",
        "            r1 = regions[i]\n",
        "            merged = False\n",
        "            for j in range(i+1, n):\n",
        "                if j in skip_indices:\n",
        "                    continue\n",
        "                r2 = regions[j]\n",
        "\n",
        "                if r1[\"type\"] == r2[\"type\"] or True:\n",
        "                    # Si quisieras mantener separado \"image\" vs \"drawing\", podrías\n",
        "                    # fusionar solo si r1[\"type\"] == r2[\"type\"]. O fusionar siempre.\n",
        "                    if rects_are_close_or_overlap(r1[\"bbox\"], r2[\"bbox\"], close_dist):\n",
        "                        # Merge them\n",
        "                        new_bbox = merge_rects(r1[\"bbox\"], r2[\"bbox\"])\n",
        "                        # Podríamos unificar el 'type'; aquí escogemos la del primero\n",
        "                        # o creamos algo como \"mixed\"\n",
        "                        new_type = r1[\"type\"] if r1[\"type\"] == r2[\"type\"] else \"mixed\"\n",
        "                        merged_list.append({\"bbox\": new_bbox, \"type\": new_type})\n",
        "                        skip_indices.add(j)\n",
        "                        merged = True\n",
        "                        if debug:\n",
        "                            print(f\"DEBUG: Merged {r1['bbox']} + {r2['bbox']} => {new_bbox}\")\n",
        "                        break\n",
        "            if not merged:\n",
        "                # No fusionamos r1 con nadie\n",
        "                merged_list.append(r1)\n",
        "\n",
        "        if len(merged_list) < len(regions):\n",
        "            # Hubo fusión => repetimos\n",
        "            regions = merged_list\n",
        "            changed = True\n",
        "        else:\n",
        "            # Sin cambio => terminamos\n",
        "            regions = merged_list\n",
        "\n",
        "    return regions\n",
        "\n",
        "\n",
        "from math import sqrt\n",
        "\n",
        "def _min_dist_between_rects(r1, r2):\n",
        "    \"\"\"\n",
        "    Calcula la distancia mínima entre dos rects (x0, y0, x1, y1)\n",
        "    si no se solapan.\n",
        "    \"\"\"\n",
        "    x0a, y0a, x1a, y1a = r1\n",
        "    x0b, y0b, x1b, y1b = r2\n",
        "\n",
        "    # Si se solapan en x, la distancia en x es 0; de lo contrario,\n",
        "    # es la diferencia entre los bordes más cercanos.\n",
        "    if x1a < x0b:\n",
        "        dx = x0b - x1a\n",
        "    elif x1b < x0a:\n",
        "        dx = x0a - x1b\n",
        "    else:\n",
        "        dx = 0\n",
        "\n",
        "    # Lo mismo para y.\n",
        "    if y1a < y0b:\n",
        "        dy = y0b - y1a\n",
        "    elif y1b < y0a:\n",
        "        dy = y0a - y1b\n",
        "    else:\n",
        "        dy = 0\n",
        "\n",
        "    # Distancia euclidiana\n",
        "    return sqrt(dx*dx + dy*dy)\n",
        "\n",
        "\n",
        "print(\"INFO: Funciones auxiliares STEM definidas (Fórmulas, Encabezados, Imágenes).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfHJhZIwUQUK"
      },
      "source": [
        "# Detección de índice\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UwU9x2dIUQUK"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def detectar_paginas_indice(pdf_path, max_paginas_a_revisar=None, umbral_min_lineas=5):\n",
        "    \"\"\"\n",
        "    Intenta detectar las páginas del índice (Tabla de Contenido) en un PDF.\n",
        "\n",
        "    Utiliza heurísticas mejoradas basadas en patrones de texto comunes,\n",
        "    combinando numeración jerárquica, palabras clave y opcionalmente\n",
        "    la presencia de números de página al final de la línea.\n",
        "\n",
        "    Args:\n",
        "        pdf_path (str): Ruta al archivo PDF.\n",
        "        max_paginas_a_revisar (int): Número máximo de páginas iniciales a revisar.\n",
        "        umbral_min_lineas (int): Mínimo de líneas de texto requeridas en una página\n",
        "                                 para siquiera considerarla como índice.\n",
        "\n",
        "    Returns:\n",
        "        list: Una lista de índices de página (basados en 0) que probablemente\n",
        "              contienen el índice. Lista vacía si no se detecta ninguno o hay error.\n",
        "    \"\"\"\n",
        "    paginas_indice_detectadas = []\n",
        "    if not os.path.exists(pdf_path):\n",
        "         print(f\"ERROR: (detectar_paginas_indice) Archivo no encontrado: {pdf_path}\")\n",
        "         return paginas_indice_detectadas\n",
        "\n",
        "    doc = None\n",
        "    try:\n",
        "        doc = fitz.open(pdf_path)\n",
        "    except Exception as e:\n",
        "        print(f\"WARN: (detectar_paginas_indice) Error al abrir PDF '{pdf_path}': {e}\")\n",
        "        return paginas_indice_detectadas\n",
        "\n",
        "    # --- Heurísticas ---\n",
        "    # Regex para numeración como 1., 1.1, 1.1.1., CAPÍTULO 1, TEMA 2, etc. (más flexible)\n",
        "    patron_numeracion_jerarquica = re.compile(\n",
        "        r\"^\\s*([0-9]+(\\.[0-9]+)*\\.?\\s+|\"  # 1., 1.1, 1.1.\n",
        "        r\"(CAP[IÍ]TULO|TEMA|SECCI[OÓ]N|PARTE)\\s+[0-9IVXLCDM]+\\b\\.?\\s*).*\",\n",
        "        re.IGNORECASE\n",
        "    )\n",
        "    # Regex para palabras clave comunes en índices/sumarios (usando search)\n",
        "    patron_palabras_clave = re.compile(\n",
        "        r\"^\\s*(INTRODUCCI[OÓ]N|PR[OÓ]LOGO|CONCLUSI[OÓ]N|EP[IÍ]LOGO|BIBLIOGRAF[IÍ]A|WEBGRAF[IÍ]A|REFERENCIAS|RESUMEN|[IÍ]NDICE|CONTENIDO|SUMARIO|ANEXO|GLOSARIO)\\b\",\n",
        "        re.IGNORECASE\n",
        "    )\n",
        "    # Regex para líneas que probablemente terminan en un número de página (puede estar precedido por puntos o espacios)\n",
        "    patron_linea_con_pagina = re.compile(r\".*[.\\s]\\s*(\\d+)\\s*$\")\n",
        "\n",
        "    num_paginas_a_escanear = min(max_paginas_a_revisar, doc.page_count)\n",
        "    posible_indice_activo = False # Flag para detectar índices multi-página\n",
        "\n",
        "    print(f\"INFO: Escaneando hasta {num_paginas_a_escanear} páginas para índice en '{os.path.basename(pdf_path)}'\")\n",
        "\n",
        "    for num_pagina in range(num_paginas_a_escanear):\n",
        "        try:\n",
        "            pagina = doc.load_page(num_pagina)\n",
        "            # Usar bloques puede ser un poco más robusto para la separación de líneas\n",
        "            bloques = pagina.get_text(\"blocks\")\n",
        "            lineas = []\n",
        "            for b in bloques:\n",
        "                # b[4] contiene el texto del bloque, puede tener \\n internos\n",
        "                block_text = b[4]\n",
        "                # Dividir por nueva línea y limpiar\n",
        "                lineas.extend(line.strip() for line in block_text.split('\\n') if line.strip())\n",
        "\n",
        "            num_total_lineas = len(lineas)\n",
        "\n",
        "            # Ignorar páginas casi vacías o portadas detectadas\n",
        "            if num_total_lineas < umbral_min_lineas or is_likely_cover(\"\\n\".join(lineas), num_pagina, doc.page_count):\n",
        "                # print(f\"DEBUG P{num_pagina+1}: Ignorada (líneas={num_total_lineas} < {umbral_min_lineas} or portada)\")\n",
        "                posible_indice_activo = False # Si no es índice, rompe la cadena\n",
        "                continue\n",
        "\n",
        "            contador_lineas_patron = 0\n",
        "            contador_palabras_clave = 0\n",
        "            contador_lineas_con_pagina = 0\n",
        "\n",
        "            for linea in lineas:\n",
        "                if patron_numeracion_jerarquica.match(linea):\n",
        "                    contador_lineas_patron += 1\n",
        "                # Usamos search para palabras clave, más flexible a indentación\n",
        "                if patron_palabras_clave.search(linea):\n",
        "                    contador_palabras_clave += 1\n",
        "                if patron_linea_con_pagina.match(linea):\n",
        "                    # Verificación adicional: asegurarse de que el número no sea parte de la numeración inicial\n",
        "                    match_num_inicial = patron_numeracion_jerarquica.match(linea)\n",
        "                    num_final_match = patron_linea_con_pagina.match(linea)\n",
        "                    if num_final_match:\n",
        "                         num_final_str = num_final_match.group(1)\n",
        "                         # Evitar contar si el número final es el mismo que el inicial (p.ej., \"1. Título 1\")\n",
        "                         if not (match_num_inicial and linea.strip().endswith(num_final_str) and len(linea.split()) < 4):\n",
        "                              contador_lineas_con_pagina += 1\n",
        "\n",
        "\n",
        "            ratio_lineas_patron = contador_lineas_patron / num_total_lineas\n",
        "            ratio_lineas_con_pagina = contador_lineas_con_pagina / num_total_lineas\n",
        "\n",
        "            # --- Lógica de Decisión Mejorada ---\n",
        "            es_pagina_indice = False\n",
        "            score = 0.0\n",
        "\n",
        "            # Puntuación base por estructura de numeración (alta importancia)\n",
        "            score += ratio_lineas_patron * 0.6\n",
        "\n",
        "            # Puntuación por líneas terminando en número (media importancia)\n",
        "            score += ratio_lineas_con_pagina * 0.3\n",
        "\n",
        "            # Bonus por presencia de palabras clave (menor importancia individual, pero ayuda)\n",
        "            if contador_palabras_clave > 0:\n",
        "                score += 0.1 # Bonus fijo pequeño si hay al menos una\n",
        "            if contador_palabras_clave > 2:\n",
        "                score += 0.1 # Bonus adicional si hay varias\n",
        "\n",
        "            # Umbral base para considerar índice\n",
        "            umbral_score_base = 0.25 # Ajustar según sea necesario\n",
        "\n",
        "            # Umbral más bajo si la página anterior fue índice (continuación)\n",
        "            umbral_score_continuacion = 0.18\n",
        "\n",
        "            if posible_indice_activo:\n",
        "                if score >= umbral_score_continuacion:\n",
        "                    es_pagina_indice = True\n",
        "            else:\n",
        "                 if score >= umbral_score_base:\n",
        "                    es_pagina_indice = True\n",
        "\n",
        "            # Refinamiento: Una página con muchas palabras clave pero CERO estructura podría ser un falso positivo\n",
        "            # O una página con ALTA estructura pero pocas líneas podría no serlo.\n",
        "            # (La comprobación de umbral_min_lineas ya ayuda con lo segundo)\n",
        "            if contador_palabras_clave > 1 and contador_lineas_patron == 0 and contador_lineas_con_pagina == 0:\n",
        "                 # Si SOLO tiene palabras clave y ninguna otra estructura, probablemente no sea índice (podría ser intro/conclusión)\n",
        "                 # A menos que tenga MUCHAS líneas con palabras clave? Podría ser un índice simple.\n",
        "                 if num_total_lineas > 10 and (contador_palabras_clave / num_total_lineas > 0.3): # Si >30% de lineas son keywords\n",
        "                     pass # Probablemente un índice simple basado en keywords, mantener es_pagina_indice si score fue suficiente\n",
        "                 else:\n",
        "                     es_pagina_indice = False # Descartar si no cumple la condición anterior\n",
        "\n",
        "\n",
        "            # DEBUGGING INTERNO\n",
        "            print(f\"  Pág {num_pagina + 1}: Lines={num_total_lineas}, \"\n",
        "                  f\"RatioPatron={ratio_lineas_patron:.2f} ({contador_lineas_patron}), \"\n",
        "                  f\"Keywords={contador_palabras_clave}, \"\n",
        "                  f\"RatioPgNum={ratio_lineas_con_pagina:.2f} ({contador_lineas_con_pagina}), \"\n",
        "                  f\"Score={score:.3f} -> Índice? {es_pagina_indice} (ActivoPrev? {posible_indice_activo})\")\n",
        "\n",
        "            if es_pagina_indice:\n",
        "                if num_pagina not in paginas_indice_detectadas:\n",
        "                     paginas_indice_detectadas.append(num_pagina)\n",
        "                posible_indice_activo = True\n",
        "            else:\n",
        "                # Si la página no cumple, se rompe la posible cadena de índice\n",
        "                posible_indice_activo = False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"WARN: (detectar_paginas_indice) Error procesando página {num_pagina} del PDF: {e}\")\n",
        "            posible_indice_activo = False # Resetear en caso de error\n",
        "            continue\n",
        "\n",
        "    if doc:\n",
        "        doc.close()\n",
        "\n",
        "    # Post-procesamiento: a veces puede detectar una página suelta entre otras.\n",
        "    # Si tenemos [0, 2], pero no 1, es menos probable que 2 sea índice.\n",
        "    # Podríamos requerir bloques contiguos, pero por simplicidad lo dejamos así por ahora.\n",
        "\n",
        "    print(f\"INFO: Páginas de índice detectadas: {[p+1 for p in paginas_indice_detectadas]}\") # Mostrar páginas base 1\n",
        "    return paginas_indice_detectadas\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agwlhCOlUQUL"
      },
      "source": [
        "# Extracción y limpieza principal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVSRPD_EUQUL"
      },
      "outputs": [],
      "source": [
        "\n",
        "def extract_and_clean_pdf_smart(pdf_path,\n",
        "                                     use_ocr_threshold=50,\n",
        "                                     language='spa', # 'language' no se usa directamente aquí, quizás en OCR\n",
        "                                     max_index_pages_to_scan=15,\n",
        "                                     max_summary_biblio_pages_to_scan=10,\n",
        "                                     debug_prints=False): # Añadido parámetro debug_prints\n",
        "    \"\"\"\n",
        "    Extrae texto de PDF, detecta fórmulas/imágenes,\n",
        "    omite portadas/índices, limpia texto, y elimina solo el bloque de\n",
        "    bibliografía sin eliminar páginas enteras.\n",
        "\n",
        "    - Aplica detect_formulas_in_text() para ver si hay LaTeX literal.\n",
        "    - Marca si el texto parece contener expresiones matemáticas (heurística).\n",
        "    - Detecta secciones matemáticas (teorema, definición, demostración, etc.)\n",
        "      a nivel global.\n",
        "    \"\"\"\n",
        "    all_formulas_detected = []\n",
        "    all_image_regions = {}\n",
        "    valid_pages_text = []\n",
        "    omitted_pages_info = []\n",
        "    # debug_prints = False # Se recibe como parámetro ahora\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(pdf_path):\n",
        "            print(f\"Error GRAVE: No se encontró el archivo PDF: {pdf_path}\")\n",
        "            # Devolver None o un dict vacío con error es mejor que solo None\n",
        "            return {\"error\": f\"File not found: {pdf_path}\", \"cleaned_text\": \"\", \"detected_formulas\": [], \"detected_image_regions\": {}, \"omitted_pages\": [], \"heuristic_math_detected\": False, \"detected_math_sections\": {}}\n",
        "\n",
        "\n",
        "        pdf_basename = os.path.basename(pdf_path)\n",
        "\n",
        "        # --- Pre-detección de ÍNDICE ---\n",
        "        if debug_prints: print(f\"DEBUG ({pdf_basename}): Pre-detectando páginas de índice (hasta {max_index_pages_to_scan} págs)...\")\n",
        "        paginas_indice_detectadas = detectar_paginas_indice(\n",
        "            pdf_path, max_paginas_a_revisar=max_index_pages_to_scan\n",
        "        )\n",
        "        if paginas_indice_detectadas:\n",
        "            if debug_prints: print(f\"DEBUG ({pdf_basename}): Posibles páginas de ÍNDICE (0-based): {paginas_indice_detectadas}\")\n",
        "\n",
        "        # --- Pre-detección de RESUMEN/BIBLIO ---\n",
        "        if debug_prints: print(f\"DEBUG ({pdf_basename}): Pre-detectando págs Resumen/Biblio (últimas {max_summary_biblio_pages_to_scan})...\")\n",
        "        paginas_resumen_detectadas, paginas_biblio_detectadas = detectar_paginas_resumen_biblio(\n",
        "            pdf_path, max_paginas_finales_a_revisar=max_summary_biblio_pages_to_scan\n",
        "        )\n",
        "        if paginas_resumen_detectadas:\n",
        "            if debug_prints: print(f\"DEBUG ({pdf_basename}): Páginas con posible RESUMEN (0-based): {paginas_resumen_detectadas}\")\n",
        "        if paginas_biblio_detectadas:\n",
        "            if debug_prints: print(f\"DEBUG ({pdf_basename}): Páginas con posible BIBLIOGRAFÍA (0-based): {paginas_biblio_detectadas}\")\n",
        "\n",
        "        # --- Crear conjunto de páginas a omitir solo para índice ---\n",
        "        # Convertir a 0-based si las funciones de detección devuelven 1-based\n",
        "        # Asumiendo que devuelven 0-based:\n",
        "        paginas_a_omitir_previamente = set(paginas_indice_detectadas)\n",
        "\n",
        "        doc = fitz.open(pdf_path)\n",
        "        num_total_pages = len(doc)\n",
        "        if debug_prints: print(f\"DEBUG ({pdf_basename}): Procesando {num_total_pages} páginas (Modo STEM).\")\n",
        "\n",
        "        for page_num in range(num_total_pages):\n",
        "            page_num_real = page_num + 1 # Para logs y referencias (1-based)\n",
        "            if debug_prints: print(f\"DEBUG ({pdf_basename}): Procesando pág {page_num_real}/{num_total_pages}...\")\n",
        "\n",
        "            # --- Omisión de páginas índice ---\n",
        "            if page_num in paginas_a_omitir_previamente:\n",
        "                reason = \"Índice (pre-detectado)\"\n",
        "                omitted_pages_info.append((page_num_real, reason))\n",
        "                if debug_prints:\n",
        "                    print(f\"  -> OMITIDA ({reason}).\")\n",
        "                continue\n",
        "\n",
        "            # --- Extracción básica de texto ---\n",
        "            page_raw_text = \"\"\n",
        "            page_raw_text_strip = \"\"\n",
        "            try:\n",
        "                page = doc.load_page(page_num) # Cargar página dentro del bucle\n",
        "                page_raw_text = page.get_text(\"text\", sort=True)\n",
        "                page_raw_text_strip = page_raw_text.strip() if page_raw_text else \"\"\n",
        "                if debug_prints and not page_raw_text_strip:\n",
        "                    print(f\"  -> WARN: get_text devolvió vacío o solo espacios.\")\n",
        "            except Exception as getTextErr:\n",
        "                print(f\"WARN ({pdf_basename}): get_text falló pág {page_num_real}: {getTextErr}.\")\n",
        "                omitted_pages_info.append((page_num_real, f\"Error get_text: {getTextErr}\"))\n",
        "                continue # Saltar página si falla la extracción básica\n",
        "\n",
        "            # --- Lógica de OCR (Opcional, si se requiere) ---\n",
        "            used_ocr = False\n",
        "            if not page_raw_text_strip or len(page_raw_text_strip) < use_ocr_threshold:\n",
        "                # Aquí iría la llamada a una función OCR si decides implementarla\n",
        "                # page_raw_text_ocr = apply_ocr_to_page(page, language=language)\n",
        "                # if page_raw_text_ocr and len(page_raw_text_ocr.strip()) > len(page_raw_text_strip):\n",
        "                #     page_raw_text = page_raw_text_ocr\n",
        "                #     page_raw_text_strip = page_raw_text.strip()\n",
        "                #     used_ocr = True\n",
        "                #     if debug_prints: print(f\"  -> INFO: OCR aplicado (resultado > {use_ocr_threshold} chars).\")\n",
        "                # elif debug_prints:\n",
        "                #     print(f\"  -> INFO: Texto < {use_ocr_threshold} chars, OCR no aplicado o sin mejora.\")\n",
        "                pass # Placeholder para OCR\n",
        "\n",
        "            # --- Comprobación de texto vacío (Post-OCR si aplica) ---\n",
        "            if not page_raw_text_strip:\n",
        "                reason = \"Sin texto válido (post-OCR)\" if used_ocr else \"Sin texto válido\"\n",
        "                omitted_pages_info.append((page_num_real, reason))\n",
        "                if debug_prints: print(f\"  -> OMITIDA ({reason}).\")\n",
        "                continue\n",
        "\n",
        "            # --- Heurística de portada ---\n",
        "            if is_likely_cover(page_raw_text_strip, page_num, num_total_pages):\n",
        "                reason = \"Portada (heurística)\"\n",
        "                omitted_pages_info.append((page_num_real, reason))\n",
        "                if debug_prints: print(f\"  -> OMITIDA ({reason}).\")\n",
        "                continue\n",
        "\n",
        "            # --- Cortar bibliografía si corresponde (Solo si la página fue pre-detectada) ---\n",
        "            final_page_text = page_raw_text_strip # Usar texto strip para búsqueda\n",
        "            original_length_before_bib_cut = len(final_page_text)\n",
        "            bibliography_cut_applied = False\n",
        "\n",
        "            if page_num in paginas_biblio_detectadas:\n",
        "                # Buscar keywords en mayúsculas para robustez, pero cortar el original\n",
        "                biblio_keywords_regex = r'^\\s*(BIBLIOGRAFÍA|REFERENCIAS|WEBGRAFÍA)\\s*$' # Más específico, inicio de línea\n",
        "                # Intentar buscar desde el final de la página hacia atrás podría ser más robusto\n",
        "                lines = final_page_text.splitlines()\n",
        "                cut_index = -1\n",
        "                for i in range(len(lines) - 1, -1, -1):\n",
        "                     if re.search(biblio_keywords_regex, lines[i].strip().upper()):\n",
        "                         # Encontrar la posición de inicio de esta línea en el texto original\n",
        "                         try:\n",
        "                            cut_index = final_page_text.rindex(lines[i])\n",
        "                            break\n",
        "                         except ValueError:\n",
        "                            pass # Seguir buscando si la línea no se encuentra exactamente\n",
        "\n",
        "                if cut_index != -1:\n",
        "                    final_page_text = final_page_text[:cut_index].strip()\n",
        "                    bibliography_cut_applied = True\n",
        "                    if debug_prints:\n",
        "                        print(f\"  -> INFO: Texto cortado por keyword de bibliografía encontrada.\")\n",
        "\n",
        "                    # Si tras cortar no queda texto, omitir la página\n",
        "                    if not final_page_text:\n",
        "                        reason = \"Texto eliminado por contenido de bibliografía\"\n",
        "                        omitted_pages_info.append((page_num_real, reason))\n",
        "                        if debug_prints: print(f\"  -> OMITIDA ({reason}).\")\n",
        "                        continue\n",
        "\n",
        "\n",
        "            # Detectar imágenes (asumiendo que la función existe)\n",
        "            page_image_bboxes = detect_image_regions_on_page(page)\n",
        "            if page_image_bboxes:\n",
        "                all_image_regions[page_num_real] = page_image_bboxes\n",
        "                if debug_prints: print(f\"  -> INFO: Detectadas {len(page_image_bboxes)} regiones de imagen.\")\n",
        "\n",
        "            # --- Página Aceptada (añadir texto final) ---\n",
        "            valid_pages_text.append(final_page_text)\n",
        "            if debug_prints: print(f\"  -> ACEPTADA (len: {len(final_page_text)} chars).\")\n",
        "\n",
        "        # --- Fin del bucle de páginas ---\n",
        "        doc.close()\n",
        "\n",
        "        # --- Resumen de omisiones ---\n",
        "        print(\"\\n\" + \"-\"*20 + f\" Resumen Omisiones ({pdf_basename}) \" + \"-\"*20)\n",
        "        if not omitted_pages_info:\n",
        "            print(\"INFO: No se omitió ninguna página.\")\n",
        "        else:\n",
        "            omitted_by_reason = defaultdict(list)\n",
        "            for page, reason in omitted_pages_info:\n",
        "                omitted_by_reason[reason].append(page)\n",
        "            print(f\"INFO: Omitidas {len(omitted_pages_info)}/{num_total_pages} páginas:\")\n",
        "            for reason, pages in sorted(omitted_by_reason.items()):\n",
        "                pages.sort()\n",
        "                # Agrupar páginas consecutivas para mejor lectura\n",
        "                grouped_pages = []\n",
        "                if pages:\n",
        "                    start_range = pages[0]\n",
        "                    end_range = pages[0]\n",
        "                    for i in range(1, len(pages)):\n",
        "                        if pages[i] == end_range + 1:\n",
        "                            end_range = pages[i]\n",
        "                        else:\n",
        "                            if start_range == end_range:\n",
        "                                grouped_pages.append(str(start_range))\n",
        "                            else:\n",
        "                                grouped_pages.append(f\"{start_range}-{end_range}\")\n",
        "                            start_range = end_range = pages[i]\n",
        "                    # Añadir el último rango/página\n",
        "                    if start_range == end_range:\n",
        "                        grouped_pages.append(str(start_range))\n",
        "                    else:\n",
        "                        grouped_pages.append(f\"{start_range}-{end_range}\")\n",
        "                print(f\"  - Razón: '{reason}', Páginas: {', '.join(grouped_pages)}\")\n",
        "        print(\"-\"*(42 + len(f\" Resumen Omisiones ({pdf_basename}) \")))\n",
        "\n",
        "        # --- Limpieza final del texto concatenado ---\n",
        "        if not valid_pages_text:\n",
        "            print(f\"ERROR ({pdf_basename}): No se aceptó ninguna página válida.\")\n",
        "            return {\n",
        "                \"cleaned_text\": \"\",\n",
        "                \"detected_formulas\": [],\n",
        "                \"detected_image_regions\": {},\n",
        "                \"omitted_pages\": omitted_pages_info,\n",
        "                \"heuristic_math_detected\": False,\n",
        "                \"detected_math_sections\": {}\n",
        "            }\n",
        "\n",
        "        full_raw_text = \"\\n\\n\".join(valid_pages_text) # Unir páginas aceptadas\n",
        "        if debug_prints: print(f\"DEBUG ({pdf_basename}): {len(valid_pages_text)} págs aceptadas. Limpiando texto concatenado...\")\n",
        "\n",
        "        # Asumiendo que clean_pdf_text_robust está definido.\n",
        "        cleaned_text = clean_pdf_text_robust(full_raw_text)\n",
        "        if debug_prints: print(f\"DEBUG ({pdf_basename}): Limpieza completada. Longitud final: {len(cleaned_text)} chars.\")\n",
        "\n",
        "        if not cleaned_text or cleaned_text.isspace():\n",
        "            print(f\"WARN ({pdf_basename}): Texto final limpio vacío.\")\n",
        "            # Devolver el estado aunque el texto esté vacío\n",
        "            return {\n",
        "                \"cleaned_text\": \"\",\n",
        "                \"detected_formulas\": all_formulas_detected,\n",
        "                \"detected_image_regions\": all_image_regions,\n",
        "                \"omitted_pages\": omitted_pages_info,\n",
        "                \"heuristic_math_detected\": False, # No hay texto para analizar\n",
        "                \"detected_math_sections\": {}\n",
        "            }\n",
        "\n",
        "\n",
        "\n",
        "        # --- Devolver Resultados ---\n",
        "        return {\n",
        "            \"cleaned_text\": cleaned_text,\n",
        "            \"detected_formulas\": all_formulas_detected,       # LaTeX literal\n",
        "            \"detected_image_regions\": all_image_regions,\n",
        "            \"omitted_pages\": omitted_pages_info,\n",
        "\n",
        "        }\n",
        "\n",
        "    except FileNotFoundError:\n",
        " # Ser más específico con la excepción\n",
        "        print(f\"Error GRAVE: No se encontró el PDF: {pdf_path}\")\n",
        "        return {\"error\": f\"File not found: {pdf_path}\", \"cleaned_text\": \"\", \"detected_formulas\": [], \"detected_image_regions\": {}, \"omitted_pages\": [], \"heuristic_math_detected\": False, \"detected_math_sections\": {}}\n",
        "    except Exception as e:\n",
        "        print(f\"Error GRAVE procesando PDF {os.path.basename(pdf_path)} ({type(e).__name__}): {e}\")\n",
        "        traceback.print_exc()\n",
        "        return {\"error\": f\"Processing error: {e}\", \"cleaned_text\": \"\", \"detected_formulas\": [], \"detected_image_regions\": {}, \"omitted_pages\": [], \"heuristic_math_detected\": False, \"detected_math_sections\": {}}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7d2RGJMbjih"
      },
      "outputs": [],
      "source": [
        "# --- Código de extracción y limpieza ---\n",
        "pdf_path = \"/content/119-2014-02-19-Carroll.AliciaEnElPaisDeLasMaravillas (1).pdf\"\n",
        "resultado = extract_and_clean_pdf_smart(pdf_path=pdf_path, debug_prints=True) # Desactiva prints internos si son muchos\n",
        "resultado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2pndiJeevRx"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "\n",
        "# Asegúrate de tener el tokenizador 'punkt'\n",
        "# nltk.download('punkt')\n",
        "\n",
        "# La función de chunking sigue siendo la misma. La incluimos aquí para que el código esté completo.\n",
        "def split_into_chunks_with_metadata(text, metadata, max_chunk_size=256, overlap_size=50):\n",
        "    if not isinstance(text, str):\n",
        "        raise ValueError(\"El texto de entrada debe ser un string.\")\n",
        "    if overlap_size >= max_chunk_size:\n",
        "        raise ValueError(\"El tamaño del solapamiento debe ser menor que el tamaño máximo del chunk.\")\n",
        "    sentences = sent_tokenize(text, language='spanish')\n",
        "    chunks = []\n",
        "    current_chunk_words = []\n",
        "    for sentence in sentences:\n",
        "        sentence_words = sentence.split()\n",
        "        if len(current_chunk_words) + len(sentence_words) > max_chunk_size and current_chunk_words:\n",
        "            chunk_text = \" \".join(current_chunk_words)\n",
        "            chunks.append({'text': chunk_text, 'metadata': metadata})\n",
        "            current_chunk_words = current_chunk_words[-overlap_size:]\n",
        "        current_chunk_words.extend(sentence_words)\n",
        "    if current_chunk_words:\n",
        "        chunk_text = \" \".join(current_chunk_words)\n",
        "        chunks.append({'text': chunk_text, 'metadata': metadata})\n",
        "    return chunks\n",
        "\n",
        "# --- NUEVA VERSIÓN DEL ORQUESTADOR ---\n",
        "def process_text_hierarchically(text_content, source_name):\n",
        "    \"\"\"\n",
        "    Procesa un texto usando una expresión regular estricta que solo coincide\n",
        "    con títulos de capítulo reales.\n",
        "    \"\"\"\n",
        "    print(\"Iniciando procesamiento con patrón regex definitivo...\")\n",
        "\n",
        "    # El patrón que solo acepta títulos en mayúsculas\n",
        "    chapter_pattern = re.compile(\n",
        "        r'^\\s*([IVXLCDM\\d]+)\\.\\s+([A-ZÁÉÍÓÚÜÑ\\d\\s:;.,\\'\"-]+)\\s*$',\n",
        "        re.MULTILINE\n",
        "    )\n",
        "\n",
        "    # Con un patrón fiable, re.split es la herramienta más limpia.\n",
        "    # El resultado será: [intro, num1, titulo1, contenido1, num2, titulo2, contenido2, ...]\n",
        "    parts = chapter_pattern.split(text_content)\n",
        "\n",
        "    all_chunks = []\n",
        "\n",
        "    # El primer elemento siempre es el texto de introducción\n",
        "    intro_text = parts[0].strip()\n",
        "    if intro_text:\n",
        "        intro_metadata = {'source': source_name, 'chapter': 'Introducción'}\n",
        "        all_chunks.extend(split_into_chunks_with_metadata(intro_text, intro_metadata))\n",
        "\n",
        "    # Procesamos el resto de las partes, que vienen en grupos de 3:\n",
        "    # (número, título, contenido).\n",
        "    for i in range(1, len(parts), 3):\n",
        "        chapter_number = parts[i].strip().replace('.', '') # Limpia puntos extra\n",
        "        chapter_title = parts[i+1].strip()\n",
        "        chapter_content = parts[i+2].strip()\n",
        "\n",
        "        if not chapter_content: continue\n",
        "\n",
        "        full_chapter_name = f\"Capítulo {chapter_number}: {chapter_title}\"\n",
        "        print(f\"  -> Capítulo detectado correctamente: {full_chapter_name}\")\n",
        "\n",
        "        chapter_metadata = {'source': source_name, 'chapter': full_chapter_name}\n",
        "        all_chunks.extend(split_into_chunks_with_metadata(chapter_content, chapter_metadata))\n",
        "\n",
        "    return all_chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6IzmSmM2iGrt"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def process_pdf_to_structured_chunks(pdf_path):\n",
        "    \"\"\"\n",
        "    Flujo de trabajo completo: extrae, hace chunking jerárquico y enriquece metadatos.\n",
        "    \"\"\"\n",
        "    print(\"--- INICIO DEL PROCESO ---\")\n",
        "\n",
        "    # 1. EXTRACCIÓN Y LIMPIEZA\n",
        "    print(\"\\n[Paso 1] Extrayendo y limpiando texto del PDF...\")\n",
        "    resultado = extract_and_clean_pdf_smart(pdf_path=pdf_path)\n",
        "    texto_limpio = resultado[\"cleaned_text\"]\n",
        "    topic = os.path.basename(pdf_path)\n",
        "    print(f\"Texto extraído. Longitud: {len(texto_limpio)} caracteres.\")\n",
        "\n",
        "    # 2. CHUNKING ESTRUCTURAL (JERÁRQUICO)\n",
        "    print(\"\\n[Paso 2] Realizando chunking jerárquico por capítulos...\")\n",
        "    structured_chunks = process_text_hierarchically(texto_limpio, source_name= 'Alicia en el pais de las maravllas')\n",
        "    print(f\"Chunking estructural completado. Se generaron {len(structured_chunks)} chunks iniciales.\")\n",
        "\n",
        "    # 3. ENRIQUECIMIENTO DE METADATOS\n",
        "    print(\"\\n[Paso 3] Enriqueciendo cada chunk con metadatos de contenido...\")\n",
        "    final_chunks_with_metadata = []\n",
        "    for i, chunk_data in enumerate(structured_chunks):\n",
        "        chunk_text = chunk_data['text']\n",
        "\n",
        "        # Obtenemos los metadatos estructurales ya existentes\n",
        "        metadata = chunk_data['metadata']\n",
        "\n",
        "\n",
        "\n",
        "        # El diccionario 'chunk_data' ahora tiene el texto y los metadatos combinados\n",
        "        final_chunks_with_metadata.append(chunk_data)\n",
        "\n",
        "    print(\"Enriquecimiento completado.\")\n",
        "    print(\"\\n--- PROCESO FINALIZADO ---\")\n",
        "    return final_chunks_with_metadata\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# EJECUCIÓN\n",
        "# ==============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Usa la ruta a tu PDF real aquí\n",
        "    pdf_path = \"/content/119-2014-02-19-Carroll.AliciaEnElPaisDeLasMaravillas (1).pdf\"\n",
        "\n",
        "    chunks = process_pdf_to_structured_chunks(pdf_path)\n",
        "\n",
        "    print(f\"\\nTotal de chunks finales generados: {len(chunks)}\\n\")\n",
        "\n",
        "    # Muestra los metadatos de cada chunk para verificar\n",
        "    print(\"=== METADATOS FINALES POR CHUNK ===\")\n",
        "    for i, chunk_meta in enumerate(chunks):\n",
        "        print(f\"\\n--- Chunk {i+1} ---\")\n",
        "        print(chunk_meta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOWTS9nHQXbG"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# --- Celda de Carga de Claves y Cliente OpenAI (Reemplaza tu bloque antiguo con este) ---\n",
        "\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# 1. Cargar las variables de entorno desde el archivo .env\n",
        "load_dotenv()\n",
        "\n",
        "# 2. Obtener la clave de API desde las variables de entorno\n",
        "#    Usamos os.getenv() para leer la variable que definimos en el archivo .env\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# 3. (MUY IMPORTANTE) Verificación de seguridad y usabilidad\n",
        "if not openai_api_key:\n",
        "    # Si la clave no se encuentra, detenemos la ejecución con un error claro.\n",
        "    raise ValueError(\"ERROR: La clave de API de OpenAI no se encontró. \"\n",
        "                     \"Asegúrate de crear un archivo '.env' en el mismo directorio que este notebook \"\n",
        "                     \"y añadir la línea: OPENAI_API_KEY='sk-...'\")\n",
        "else:\n",
        "    print(\"✅ Clave de API de OpenAI cargada exitosamente desde el archivo .env.\")\n",
        "\n",
        "# 4. Inicializa el cliente usando la clave cargada de forma segura\n",
        "client = OpenAI(\n",
        "    api_key=openai_api_key\n",
        ")\n",
        "\n",
        "print(\"✅ Cliente de OpenAI inicializado.\")\n",
        "\n",
        "# --- Tu bloque de prueba (sin cambios, sigue siendo útil) ---\n",
        "try:\n",
        "    print(\"\\nRealizando una llamada de prueba a la API de embeddings...\")\n",
        "    response = client.embeddings.create(\n",
        "        model=\"text-embedding-3-small\",\n",
        "        input=[\"Este es un texto de prueba para generar un embedding.\"]\n",
        "    )\n",
        "    embedding_vector = response.data[0].embedding\n",
        "    print(\"✅ Embedding generado exitosamente.\")\n",
        "    print(f\"   El embedding tiene {len(embedding_vector)} dimensiones.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Ocurrió un error al contactar la API de OpenAI: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWzRqjnftIBO"
      },
      "outputs": [],
      "source": [
        "%pip install faiss-cpu langchain_openai rank_bm25 langchain_google_genai sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKtBUaPJ5yI_"
      },
      "outputs": [],
      "source": [
        "  # Install the faiss library\n",
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# --- FUNCIÓN PARA GENERAR EMBEDDINGS ---\n",
        "\n",
        "def generate_embeddings_for_chunks(chunks_with_metadata, batch_size=50, model=\"text-embedding-3-small\"):\n",
        "    \"\"\"\n",
        "    Genera embeddings para una lista de chunks de texto usando la API de OpenAI en lotes.\n",
        "\n",
        "    Args:\n",
        "        chunks_with_metadata (list[dict]): La lista de chunks, donde cada elemento\n",
        "                                           es un diccionario {'text': ..., 'metadata': ...}.\n",
        "        batch_size (int): El número de textos a procesar en cada llamada a la API.\n",
        "        model (str): El modelo de embedding de OpenAI a utilizar.\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: La misma lista de chunks, pero ahora cada diccionario también\n",
        "                    contiene una clave 'embedding' con su vector numérico.\n",
        "    \"\"\"\n",
        "    chunks_with_embeddings = []\n",
        "\n",
        "    # Iteramos sobre la lista de chunks en lotes del tamaño de 'batch_size'\n",
        "    for i in range(0, len(chunks_with_metadata), batch_size):\n",
        "        # 1. Selecciona el lote actual de chunks\n",
        "        current_batch = chunks_with_metadata[i:i + batch_size]\n",
        "\n",
        "        # 2. Extrae solo el texto de cada chunk en el lote\n",
        "        texts_to_embed = [chunk['text'] for chunk in current_batch]\n",
        "\n",
        "        print(f\"Procesando lote {i//batch_size + 1}/{(len(chunks_with_metadata) - 1)//batch_size + 1}... \"\n",
        "              f\"({len(texts_to_embed)} textos)\")\n",
        "\n",
        "        try:\n",
        "            # 3. Llama a la API de OpenAI con el lote de textos\n",
        "            response = client.embeddings.create(\n",
        "                model=model,\n",
        "                input=texts_to_embed\n",
        "            )\n",
        "\n",
        "            # 4. Extrae los embeddings de la respuesta\n",
        "            embeddings = [item.embedding for item in response.data]\n",
        "\n",
        "            # 5. Asigna cada embedding a su chunk correspondiente\n",
        "            for j, chunk in enumerate(current_batch):\n",
        "                chunk['embedding'] = embeddings[j] # Añade la nueva clave 'embedding'\n",
        "                chunks_with_embeddings.append(chunk)\n",
        "\n",
        "            # Pausa opcional para no exceder los límites de la API (rate limits)\n",
        "            time.sleep(1) # Pausa de 1 segundo entre lotes\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error procesando el lote que empieza en el índice {i}: {e}\")\n",
        "            # Opcional: podrías decidir saltar este lote y continuar, o detener el proceso\n",
        "            continue\n",
        "\n",
        "    return chunks_with_embeddings\n",
        "\n",
        "\n",
        "# --- EJEMPLO DE USO ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Simula tu lista de chunks ya procesada (usa solo unos pocos para el ejemplo)\n",
        "    # En tu código real, usarías la lista completa de 143 chunks.\n",
        "    sample_chunks_processed = [\n",
        "        {'text': 'Alicia empezaba ya a cansarse de estar sentada con su hermana a la orilla del río, sin tener nada que hacer...',\n",
        "         'metadata': {'source': 'Alicia.pdf', 'chapter': 'Capítulo I: EN LA MADRIGUERA DEL CONEJO'}},\n",
        "        {'text': '...cuando de pronto saltó cerca de ella un Conejo Blanco de ojos rosados. No había nada muy extraordinario en esto...',\n",
        "         'metadata': {'source': 'Alicia.pdf', 'chapter': 'Capítulo I: EN LA MADRIGUERA DEL CONEJO'}},\n",
        "        {'text': '—¡Curiorífico y curiorífico! —exclamó Alicia. ¡Ahora me estoy estirando como el telescopio más largo que haya existido jamás!',\n",
        "         'metadata': {'source': 'Alicia.pdf', 'chapter': 'Capítulo II: EL CHARCO DE LÁGRIMAS'}},\n",
        "        # ... y así sucesivamente para todos tus chunks\n",
        "    ]\n",
        "\n",
        "    print(f\"Se van a procesar {len(sample_chunks_processed)} chunks de ejemplo.\")\n",
        "\n",
        "    # 2. Llama a la función para generar los embeddings\n",
        "    # (Usa un batch_size pequeño para este ejemplo)\n",
        "    chunks_final_data = generate_embeddings_for_chunks(chunks, batch_size=2)\n",
        "\n",
        "    # 3. Verifica el resultado\n",
        "    print(\"\\n--- PROCESO DE EMBEDDING COMPLETADO ---\")\n",
        "    if chunks_final_data:\n",
        "        print(f\"Se generaron embeddings para {len(chunks_final_data)} chunks.\")\n",
        "\n",
        "        # Imprime el primer chunk para ver la nueva estructura\n",
        "        print(\"\\nEjemplo del primer chunk con su embedding:\")\n",
        "        first_chunk = chunks_final_data[0]\n",
        "\n",
        "        # Usamos pprint para una mejor visualización\n",
        "        print({\n",
        "            'text': first_chunk['text'][:50] + '...', # Muestra solo el inicio del texto\n",
        "            'metadata': first_chunk['metadata'],\n",
        "            'embedding': f\"[Vector de {len(first_chunk['embedding'])} dimensiones]\" # Muestra un resumen del embedding\n",
        "        })\n",
        "    else:\n",
        "        print(\"No se generaron embeddings.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldvnKNO1qtlS"
      },
      "outputs": [],
      "source": [
        "# --- Añade esto al final de tu script de generación de embeddings ---\n",
        "\n",
        "import numpy as np\n",
        "import faiss\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Suponiendo que 'chunks_final_data' es tu lista de chunks con texto, metadatos y embeddings\n",
        "\n",
        "# --- CONFIGURACIÓN DE NOMBRES DE ARCHIVO ---\n",
        "# Usemos nombres que correspondan a nuestro nuevo libro\n",
        "INDEX_PATH = \"alicia.index\"\n",
        "TEXTS_PATH = \"alicia_texts.pkl\"\n",
        "METAS_PATH = \"alicia_metas.pkl\"\n",
        "\n",
        "# 1. Separar los datos para guardarlos\n",
        "embeddings = np.array([chunk['embedding'] for chunk in chunks_final_data], dtype=np.float32)\n",
        "texts = [chunk['text'] for chunk in chunks_final_data]\n",
        "metadatas = [chunk['metadata'] for chunk in chunks_final_data]\n",
        "\n",
        "print(f\"Datos separados: {len(embeddings)} embeddings, {len(texts)} textos, {len(metadatas)} metadatos.\")\n",
        "print(f\"Dimensiones del vector de embedding: {embeddings.shape[1]}\")\n",
        "\n",
        "# 2. Crear y entrenar el índice FAISS\n",
        "# Usamos un índice simple 'IndexFlatL2' que es bueno para empezar\n",
        "d = embeddings.shape[1]  # Dimensión de los vectores\n",
        "index = faiss.IndexFlatL2(d)\n",
        "print(f\"Índice FAISS vacío creado con dimensión {d}.\")\n",
        "\n",
        "# Añadir los vectores al índice\n",
        "index.add(embeddings)\n",
        "print(f\"Se han añadido {index.ntotal} vectores al índice FAISS.\")\n",
        "\n",
        "# 3. Guardar todo en archivos\n",
        "print(f\"Guardando índice FAISS en '{INDEX_PATH}'...\")\n",
        "faiss.write_index(index, INDEX_PATH)\n",
        "\n",
        "print(f\"Guardando textos en '{TEXTS_PATH}'...\")\n",
        "with open(TEXTS_PATH, 'wb') as f:\n",
        "    pickle.dump(texts, f)\n",
        "\n",
        "print(f\"Guardando metadatos en '{METAS_PATH}'...\")\n",
        "with open(METAS_PATH, 'wb') as f:\n",
        "    pickle.dump(metadatas, f)\n",
        "\n",
        "print(\"\\n--- ¡Proceso de guardado completado! ---\")\n",
        "print(\"Ahora puedes usar estos 3 archivos en tu script de búsqueda RAG.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_A5gi7ZHo2Hm"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "drive.mount('/content/drive')\n",
        "os.chdir('/content/drive/MyDrive/Alicia-RAG-Chatbot') # Ajusta esta ruta si es necesario"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vuN1gBeRXK2c"
      },
      "outputs": [],
      "source": [
        "# Ejemplo para cargar claves en un notebook\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "\n",
        "if not OPENAI_API_KEY or not GOOGLE_API_KEY:\n",
        "    print(\"⚠️ ADVERTENCIA: No se encontraron las claves de API en el entorno.\")\n",
        "    print(\"Asegúrate de tener un archivo .env o de haber configurado los secretos de Colab.\")\n",
        "else:\n",
        "    print(\"✅ Claves de API cargadas exitosamente.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ocCTci55yI_"
      },
      "outputs": [],
      "source": [
        "# --- Celda 2: Parámetros de Configuración y Variables Globales ---\n",
        "\n",
        "# --- Parámetros de Archivos y Azure ---\n",
        "INDEX_PATH = \"alicia.index\"\n",
        "TEXTS_PATH = \"alicia_texts.pkl\"\n",
        "METAS_PATH = \"alicia_metas.pkl\"\n",
        "AZURE_EMBEDDING_DEPLOYMENT_NAME = \"text-embedding-3-small\" # Asegúrate que coincide con tu deployment\n",
        "AZURE_API_VERSION = \"2024-02-01\" # O la versión que uses ej: \"2023-05-15\"\n",
        "\n",
        "# --- Parámetros para Búsqueda Híbrida y Reranking ---\n",
        "K_FAISS_INITIAL = 100  # Número de candidatos a recuperar de FAISS\n",
        "K_BM25_INITIAL = 100   # Número de candidatos a recuperar de BM25\n",
        "K_RERANK = 80         # Número de candidatos a pasar al reranker (<= K_FAISS + K_BM25)\n",
        "K_FINAL = 3\n",
        "USE_DYNAMIC_K = True        # True para usar K dinámico, False para usar K_FINAL fijo\n",
        "RERANKER_SCORE_THRESHOLD = 1.5 # Umbral mínimo para considerar un chunk (ajustar según scores observados)\n",
        "MIN_CHUNKS_DYNAMIC = 3      # Mínimo de chunks a devolver si USE_DYNAMIC_K es True\n",
        "MAX_CHUNKS_DYNAMIC = 7      # Máximo de chunks a devolver si USE_DYNAMIC_K es True         # Número final de chunks a devolver al LLM            # Número final de chunks a devolver al LLM\n",
        "RERANKER_MODEL = 'cross-encoder/ms-marco-MiniLM-L-12-v2' # Modelo CrossEncoder\n",
        "\n",
        "# --- Variables Globales para inicialización única (se llenarán en la primera ejecución) ---\n",
        "is_retriever_initialized = False\n",
        "# Objetos principales:\n",
        "embeddings_model = None\n",
        "faiss_index = None\n",
        "texts = None\n",
        "metadatas = None\n",
        "bm25 = None\n",
        "reranker = None\n",
        "# Opcional: stop words en español si usas NLTK\n",
        "# spanish_stopwords = stopwords.words('spanish')\n",
        "\n",
        "print(\"INFO: Celda 2 - Parámetros y Variables Globales definidas.\")\n",
        "print(f\"  - K_FAISS_INITIAL: {K_FAISS_INITIAL}\")\n",
        "print(f\"  - K_BM25_INITIAL: {K_BM25_INITIAL}\")\n",
        "print(f\"  - K_RERANK: {K_RERANK}\")\n",
        "print(f\"  - K_FINAL: {K_FINAL}\")\n",
        "print(f\"  - RERANKER_MODEL: {RERANKER_MODEL}\")\n",
        "print(\"--- Fin Celda 2 ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWQVy_lU5yI_"
      },
      "outputs": [],
      "source": [
        "# --- Celda 3: Funciones Auxiliares ---\n",
        "\n",
        "def simple_tokenizer(text):\n",
        "    \"\"\"Tokenizador simple: minúsculas y split por espacios.\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return []\n",
        "    return text.lower().split()\n",
        "\n",
        "# Opcional: Tokenizador más robusto con NLTK (requiere descargas en Celda 1)\n",
        "# def nltk_tokenizer(text):\n",
        "#     \"\"\"Tokenizador con NLTK: minúsculas, palabras, sin puntuación ni stopwords.\"\"\"\n",
        "#     if not isinstance(text, str):\n",
        "#         return []\n",
        "#     words = word_tokenize(text.lower(), language='spanish')\n",
        "#     # Asegúrate que spanish_stopwords está definida si descomentas esto\n",
        "#     # return [word for word in words if word.isalnum() and word not in spanish_stopwords]\n",
        "#     return [word for word in words if word.isalnum()] # Sin stopwords\n",
        "\n",
        "# Elige tu tokenizador preferido aquí (¡asegúrate que la función existe!)\n",
        "tokenizer_for_bm25 = simple_tokenizer\n",
        "# tokenizer_for_bm25 = nltk_tokenizer # Si prefieres NLTK\n",
        "\n",
        "\n",
        "def norm_score(score, min_val, max_val):\n",
        "    \"\"\"\n",
        "    Normaliza un score a un rango [0, 1].\n",
        "    Maneja el caso donde min_val == max_val para evitar división por cero.\n",
        "    \"\"\"\n",
        "    if min_val == max_val:\n",
        "        # Si todos los scores son iguales, podemos devolver 0.5 (neutral) o 1 si el score es ese valor, o 0.\n",
        "        # Devolver 0 si min_val == max_val y score == min_val (o cualquier score ya que son todos iguales)\n",
        "        # o 0.5 para indicar que no hay varianza. Elegiremos 0.5 como un valor neutral.\n",
        "        # Otra opción es devolver 1.0 si solo hay un resultado y es positivo, o 0.0 si es 0.\n",
        "        # O, si solo hay un elemento, su score normalizado puede ser 1.\n",
        "        return 1.0 if score > 0 else 0.0 # Si hay un solo score y es > 0, es el \"mejor\"\n",
        "    if max_val - min_val == 0: # Otra forma de chequear división por cero\n",
        "        return 0.5 # O 1.0 si el score es el único valor\n",
        "    return (score - min_val) / (max_val - min_val)\n",
        "\n",
        "import re\n",
        "\n",
        "def calcular_pesos_dinamicos(query: str, subject: str = None) -> tuple[float, float]:\n",
        "    \"\"\"\n",
        "    Analiza la query educativa y el tema (opcional) y ajusta pesos entre BM25 y Embeddings.\n",
        "    Devuelve (peso_bm25, peso_emb).\n",
        "    \"\"\"\n",
        "    query_lower = query.lower()\n",
        "    query_original = query # Para checks de mayúsculas\n",
        "\n",
        "    # --- Pesos Base ---\n",
        "    peso_bm25 = 0.4\n",
        "    peso_emb = 0.6\n",
        "    razon_principal = \"Default (ligero sesgo Embedding)\"\n",
        "    detalles_razon = []\n",
        "\n",
        "    # --- 1. Indicadores de ALTA ESPECIFICIDAD (Prioridad Alta para BM25) ---\n",
        "\n",
        "    # 1.1. Citas exactas (texto entre comillas)\n",
        "    if re.search(r'\"[^\"]+\"', query_original): # Busca texto entre comillas dobles\n",
        "        peso_bm25 = 0.85\n",
        "        peso_emb = 0.15\n",
        "        razon_principal = \"Cita Exacta\"\n",
        "        detalles_razon.append(\"BM25 priorizado para coincidencia literal.\")\n",
        "        print_pesos_info(razon_principal, detalles_razon, peso_bm25, peso_emb)\n",
        "        return peso_bm25, peso_emb\n",
        "\n",
        "    # 1.bis. Definición de Término Clave Específico (Ej: \"elipsis\", \"hipérbaton\")\n",
        "    definicion_keywords_specific_term = [\n",
        "        \"define\", \"definición de\", \"definir\", \"significa\",\n",
        "        \"qué es\", \"que es\", \"cuál es el significado de\",\n",
        "        \"concepto de\"\n",
        "    ]\n",
        "    term_to_define_specific = \"\"\n",
        "    for keyword in definicion_keywords_specific_term:\n",
        "        # Patrón para \"keyword X\" o \"keyword 'X'\" o \"keyword \"X\"\"\n",
        "        # o para \"X keyword\" (menos común para estas keywords pero podría pasar)\n",
        "        # Priorizamos \"keyword X\"\n",
        "        if query_lower.startswith(keyword + \" \"):\n",
        "            potential_term = query_lower[len(keyword)+1:].strip()\n",
        "            # Quitar comillas y signos de interrogación del término\n",
        "            potential_term = re.sub(r\"['\\\"?¿!¡]$\", \"\", potential_term).strip()\n",
        "            potential_term = re.sub(r\"^['\\\"]\", \"\", potential_term).strip()\n",
        "\n",
        "            # Si la query original tenía el término entre comillas, es buena señal\n",
        "            if f\"'{potential_term}'\" in query_original or f'\"{potential_term}\"' in query_original:\n",
        "                 term_to_define_specific = potential_term\n",
        "                 break\n",
        "            # Si no, tomarlo si es corto\n",
        "            elif len(potential_term.split()) <= 3:\n",
        "                 term_to_define_specific = potential_term\n",
        "                 break\n",
        "\n",
        "    if term_to_define_specific and len(term_to_define_specific.split()) <= 3 and len(query.split()) < 8 : # Término corto, query no demasiado larga\n",
        "        # Evitar que una pregunta conceptual larga que casualmente empieza con \"qué es la vida...\" caiga aquí\n",
        "        # Si la query es más larga, es probable que sea más conceptual.\n",
        "        peso_bm25 = 0.80 # Alta prioridad para BM25 para encontrar el término exacto\n",
        "        peso_emb = 0.20\n",
        "        razon_principal = \"Definición de Término Clave Específico\"\n",
        "        detalles_razon.append(f\"Término detectado: '{term_to_define_specific}'. BM25 fuertemente priorizado.\")\n",
        "        print_pesos_info(razon_principal, detalles_razon, peso_bm25, peso_emb)\n",
        "        return peso_bm25, peso_emb\n",
        "\n",
        "\n",
        "    # 1.2. Búsqueda de Leyes, Artículos, Teoremas específicos\n",
        "    if re.search(r'\\b(ley|artículo|teorema|postulado|axioma|principio)\\s+([0-9]+|[xviíclmd]+|[A-Za-z\\s]+)\\b', query_lower, re.IGNORECASE):\n",
        "        peso_bm25 = 0.75\n",
        "        peso_emb = 0.25\n",
        "        razon_principal = \"Ley/Artículo/Teorema Específico\"\n",
        "        detalles_razon.append(\"BM25 priorizado para identificadores exactos.\")\n",
        "        print_pesos_info(razon_principal, detalles_razon, peso_bm25, peso_emb)\n",
        "        return peso_bm25, peso_emb\n",
        "\n",
        "    # 1.3. Fórmulas o Ecuaciones\n",
        "    if re.search(r'\\b[a-zA-Z]\\s*=\\s*[a-zA-Z0-9]|\\b[a-zA-Z]\\w*\\([a-zA-Z\\d,\\s]*\\)|[a-zA-Z]\\w*_[a-zA-Z\\d]|\\w\\^[2-9]\\b', query_original):\n",
        "        if subject in [\"Física\", \"Biología\", \"Matemáticas\", \"Química\"]: # Más probable que sea una fórmula\n",
        "            peso_bm25 = 0.70\n",
        "            peso_emb = 0.30\n",
        "            razon_principal = \"Posible Fórmula/Ecuación\"\n",
        "            detalles_razon.append(f\"BM25 priorizado en {subject} para coincidencia estructural.\")\n",
        "            print_pesos_info(razon_principal, detalles_razon, peso_bm25, peso_emb)\n",
        "            return peso_bm25, peso_emb\n",
        "\n",
        "    # --- 2. Indicadores de ESPECIFICIDAD MEDIA (Favorecen BM25, pero con espacio para semántica) ---\n",
        "\n",
        "    # 2.1. Nombres Propios\n",
        "    nombres_propios_candidatos = re.findall(r'\\b[A-ZÁÉÍÓÚÑ][a-záéíóúñ]{2,}(?:\\s+[A-ZÁÉÍÓÚÑ][a-záéíóúñ]{1,})*\\b', query_original)\n",
        "    if nombres_propios_candidatos:\n",
        "        if not (len(nombres_propios_candidatos) == 1 and query_original.startswith(nombres_propios_candidatos[0]) and len(query.split()) > 3):\n",
        "            peso_bm25 = max(peso_bm25, 0.65) # Aumenta si el default era menor, o lo establece\n",
        "            peso_emb = 1.0 - peso_bm25\n",
        "            if razon_principal.startswith(\"Default\"): razon_principal = \"Nombre Propio Detectado\"\n",
        "            detalles_razon.append(f\"Candidatos NP: {nombres_propios_candidatos}. BM25 priorizado.\")\n",
        "\n",
        "    # 2.2. Fechas, Años, Siglos\n",
        "    if re.search(r'\\b\\d{3,4}\\b', query_lower) or \\\n",
        "       re.search(r'\\bsiglo\\s+(?:[xviíclmd]+|[0-9]+)\\b', query_lower) or \\\n",
        "       re.search(r'\\b(año|fecha)\\s+\\d{1,4}\\b', query_lower) or \\\n",
        "       re.search(r'\\b\\d{1,2}(?:/| de |-| del )\\w+(?:/| de |-| del )\\d{2,4}\\b', query_lower):\n",
        "        peso_bm25 = max(peso_bm25, 0.70)\n",
        "        peso_emb = 1.0 - peso_bm25\n",
        "        if razon_principal.startswith(\"Default\") or \"Nombre Propio\" in razon_principal: razon_principal = \"Fecha/Año/Siglo Detectado\"\n",
        "        detalles_razon.append(\"BM25 priorizado para especificidad temporal.\")\n",
        "        if subject == \"Historia\":\n",
        "            peso_bm25 = max(peso_bm25, 0.75) # Aún más para Historia\n",
        "            peso_emb = 1.0 - peso_bm25\n",
        "            detalles_razon.append(\"Alta prioridad BM25 en Historia.\")\n",
        "\n",
        "    # 2.3. Acrónimos y Términos Técnicos Muy Específicos\n",
        "    acronimos_candidatos = re.findall(r'\\b[A-ZÁÉÍÓÚÑ]{2,}\\b', query_original)\n",
        "    if acronimos_candidatos and not query_original.isupper():\n",
        "        if not (len(acronimos_candidatos) == 1 and query_original.startswith(acronimos_candidatos[0])):\n",
        "            peso_bm25 = max(peso_bm25, 0.60)\n",
        "            peso_emb = 1.0 - peso_bm25\n",
        "            if razon_principal.startswith(\"Default\") or \"Nombre Propio\" in razon_principal or \"Fecha\" in razon_principal:\n",
        "                razon_principal = \"Acrónimo/Término Técnico Específico Detectado\"\n",
        "            detalles_razon.append(f\"Candidatos Acrónimo: {acronimos_candidatos}. BM25 con peso incrementado.\")\n",
        "\n",
        "\n",
        "    # --- 3. Indicadores de BÚSQUEDA DE DEFINICIONES (Equilibrio, si no es ya muy específico) ---\n",
        "    # Esta regla se aplica si las de ALTA ESPECIFICIDAD (incluida 1.bis) no se activaron y retornaron.\n",
        "    definicion_keywords_general = [\"define\", \"definición de\", \"definir\", \"significa\", \"concepto de\"]\n",
        "    que_es_keywords_general = [\"qué es\", \"que es\", \"cual es el significado de\", \"cuál es el significado de\"]\n",
        "\n",
        "    is_general_definition_request = False\n",
        "    if any(keyword in query_lower for keyword in definicion_keywords_general) or \\\n",
        "       any(query_lower.startswith(keyword) for keyword in que_es_keywords_general):\n",
        "        is_general_definition_request = True\n",
        "\n",
        "    if is_general_definition_request:\n",
        "        # Si ya se marcó como muy específico (nombre propio, fecha, acrónimo), mantenemos BM25 alto,\n",
        "        # pero si la razón principal aún es \"Default\" o algo menos específico.\n",
        "        if peso_bm25 < 0.6: # Solo ajusta si no es ya específico por reglas anteriores\n",
        "            peso_bm25 = 0.55\n",
        "            peso_emb = 0.45\n",
        "            razon_principal = \"Petición de Definición General\"\n",
        "            detalles_razon.append(\"Pesos ligeramente inclinados a BM25 para literalidad, pero con semántica.\")\n",
        "        else:\n",
        "            detalles_razon.append(\"Petición de definición, pero query ya tenía especificidad media/alta.\")\n",
        "\n",
        "\n",
        "    # --- 4. Indicadores de CONCEPTUALIDAD (Prioridad para Embeddings) ---\n",
        "    concept_keywords_strong = [\"explica\", \"describe el proceso de\", \"analiza las causas de\", \"compara y contrasta\",\n",
        "                               \"cuál es la importancia de\", \"interpreta\", \"relación entre\", \"impacto de\",\n",
        "                               \"evolución de\", \"fundamentos de\", \"teoría de\"]\n",
        "    concept_keywords_medium = [\"cómo funciona\", \"por qué ocurre\", \"cuáles son las características\",\n",
        "                               \"tipos de\", \"función de\", \"origen de\", \"propiedades de\"]\n",
        "\n",
        "    is_conceptual = False\n",
        "    conceptual_keyword_found = \"\"\n",
        "    for keyword in concept_keywords_strong:\n",
        "        if keyword in query_lower:\n",
        "            is_conceptual = True\n",
        "            conceptual_keyword_found = keyword\n",
        "            detalles_razon.append(f\"Palabra clave conceptual fuerte detectada: '{keyword}'.\")\n",
        "            break\n",
        "    if not is_conceptual:\n",
        "        for keyword in concept_keywords_medium:\n",
        "            if keyword in query_lower:\n",
        "                is_conceptual = True\n",
        "                conceptual_keyword_found = keyword\n",
        "                detalles_razon.append(f\"Palabra clave conceptual media detectada: '{keyword}'.\")\n",
        "                break\n",
        "\n",
        "    if is_conceptual:\n",
        "        # Si es una pregunta conceptual sobre un término muy específico (ya capturado por NP, Fecha, Acrónimo)\n",
        "        # Ej: \"Explica el impacto de la Peste Negra\" -> Peste Negra (NP) + Explica (Conceptual)\n",
        "        if peso_bm25 >= 0.65 : # Ya era muy específico\n",
        "            peso_bm25 = 0.55 # Mantenemos algo de BM25 para el término, pero damos espacio a la explicación\n",
        "            peso_emb = 0.45\n",
        "            razon_principal = \"Pregunta Conceptual Muy Específica\"\n",
        "            detalles_razon.append(f\"Término específico combinado con petición conceptual ('{conceptual_keyword_found}').\")\n",
        "        elif peso_bm25 >= 0.55 and peso_bm25 < 0.65: # Especificidad media\n",
        "            peso_bm25 = 0.40\n",
        "            peso_emb = 0.60\n",
        "            razon_principal = \"Pregunta Conceptual con Especificidad Media\"\n",
        "            detalles_razon.append(f\"Término con especificidad media combinado con petición conceptual ('{conceptual_keyword_found}').\")\n",
        "        else: # Pregunta conceptual más general\n",
        "            peso_bm25 = 0.25\n",
        "            peso_emb = 0.75\n",
        "            razon_principal = \"Pregunta Conceptual General\"\n",
        "            detalles_razon.append(f\"Mayor peso para Embeddings debido a '{conceptual_keyword_found}'.\")\n",
        "\n",
        "\n",
        "    # --- 5. Ajustes por Asignatura (si se proporciona y no hay una regla fuerte dominante) ---\n",
        "    if subject and (razon_principal.startswith(\"Default\") or \"Petición de Definición General\" in razon_principal):\n",
        "        original_razon_principal = razon_principal # Guardar por si no se modifica\n",
        "        if subject == \"Lengua Castellana\":\n",
        "            if \"analiza el poema\" in query_lower or \"figuras retóricas\" in query_lower or \"estilo de\" in query_lower or \"comentario de texto\" in query_lower:\n",
        "                peso_bm25 = 0.3\n",
        "                peso_emb = 0.7\n",
        "                razon_principal = f\"Conceptual (Lengua - Análisis Literario)\"\n",
        "            elif \"regla gramatical\" in query_lower or \"ortografía de\" in query_lower or \"sintaxis de\" in query_lower:\n",
        "                peso_bm25 = 0.6\n",
        "                peso_emb = 0.4\n",
        "                razon_principal = f\"Específico (Lengua - Gramática/Ortografía)\"\n",
        "        elif subject == \"Historia\":\n",
        "            if \"batalla de\" in query_lower or \"tratado de\" in query_lower or \"reinado de\" in query_lower or \"guerra de\" in query_lower:\n",
        "                if peso_bm25 < 0.65: # Solo si no fue ya capturado por NP/Fecha con alta prioridad\n",
        "                    peso_bm25 = 0.65\n",
        "                    peso_emb = 0.35\n",
        "                    razon_principal = f\"Evento Específico (Historia)\"\n",
        "\n",
        "        if original_razon_principal != razon_principal: # Si se aplicó una regla de asignatura\n",
        "             detalles_razon.append(f\"Ajuste por asignatura '{subject}'.\")\n",
        "\n",
        "\n",
        "    # --- 6. Ajuste final por longitud de la query (si aún es default o poco definido) ---\n",
        "    # Se aplica si ninguna regla fuerte o de especificidad media/conceptual clara dominó\n",
        "    if razon_principal.startswith(\"Default\") or \\\n",
        "       (\"Petición de Definición General\" in razon_principal and peso_bm25 == 0.55) or \\\n",
        "       (peso_bm25 >= 0.35 and peso_bm25 <= 0.45 and not is_conceptual): # Default o ligeramente inclinado a Emb sin ser conceptual fuerte\n",
        "\n",
        "        num_words_query = len(query.split())\n",
        "        if num_words_query > 10:\n",
        "            peso_bm25 = 0.30\n",
        "            peso_emb = 0.70\n",
        "            razon_principal = \"Ajuste por Longitud (Larga -> Conceptual)\"\n",
        "            detalles_razon.append(f\"Query larga ({num_words_query} palabras), favoreciendo semántica.\")\n",
        "        elif num_words_query < 4:\n",
        "            peso_bm25 = 0.50 # Si era default (0.4), lo sube un poco para términos cortos\n",
        "            peso_emb = 0.50\n",
        "            razon_principal = \"Ajuste por Longitud (Corta -> Equilibrio/Específica)\"\n",
        "            detalles_razon.append(f\"Query corta ({num_words_query} palabras), buscando equilibrio o término.\")\n",
        "\n",
        "\n",
        "    print_pesos_info(razon_principal, detalles_razon, peso_bm25, peso_emb)\n",
        "    return peso_bm25, peso_emb\n",
        "\n",
        "def print_pesos_info(razon_principal, detalles_razon, peso_bm25, peso_emb):\n",
        "    \"\"\"Función auxiliar para imprimir la información de los pesos.\"\"\"\n",
        "    print(f\"  INFO DinamicWeights: Razón Principal = {razon_principal}\")\n",
        "    if detalles_razon:\n",
        "        for detalle in detalles_razon:\n",
        "            print(f\"    - {detalle}\")\n",
        "    print(f\"  INFO DinamicWeights: Pesos Asignados -> BM25={peso_bm25:.2f}, Embedding={peso_emb:.2f}\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"INFO: Celda 3 - Funciones auxiliares definidas (tokenizer, pesos, normalización).\")\n",
        "print(f\"  - Usando tokenizer: {tokenizer_for_bm25.__name__}\")\n",
        "print(\"--- Fin Celda 3 ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q5EI_O0Kw55R"
      },
      "outputs": [],
      "source": [
        "topic = 'Alicia en el pais de las maravillas'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mo63UKyZ5yJA"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Asumo que las importaciones necesarias como numpy, faiss, pickle, etc., ya están en tu archivo.\n",
        "# Asegúrate de importar la clase correcta:\n",
        "from rank_bm25 import BM25Okapi\n",
        "from langchain_openai import OpenAIEmbeddings # <--- CAMBIO: Importar esta clase\n",
        "from sentence_transformers import CrossEncoder\n",
        "# ... (resto de tus importaciones y variables globales como is_retriever_initialized)\n",
        "\n",
        "# Asumo que las importaciones y variables globales ya están definidas antes de esta función.\n",
        "# Librerías necesarias:\n",
        "# import faiss, pickle, os, traceback\n",
        "# import numpy as np\n",
        "# from rank_bm25 import BM25Okapi\n",
        "# from langchain_openai import OpenAIEmbeddings\n",
        "# from sentence_transformers import CrossEncoder\n",
        "\n",
        "def my_hybrid_rerank_retriever(query: str) -> str:\n",
        "    \"\"\"\n",
        "    Función retriever completa que usa búsqueda híbrida (FAISS + BM25), fusión de scores,\n",
        "    reranking con CrossEncoder y devuelve el contexto final como un string.\n",
        "    Carga todos los recursos necesarios en la primera llamada.\n",
        "    \"\"\"\n",
        "    # Las variables globales se acceden y modifican aquí\n",
        "    global is_retriever_initialized, embeddings_model, faiss_index, texts, metadatas, bm25, reranker\n",
        "\n",
        "    # --- Bloque de Inicialización (se ejecuta solo la primera vez) ---\n",
        "    if not is_retriever_initialized:\n",
        "        print(\"INFO: Inicializando el retriever HÍBRIDO por primera vez...\")\n",
        "        try:\n",
        "            # --- SECCIÓN CORREGIDA ---\n",
        "            # 1. Cargar modelo de Embedding de OpenAI (CON INDENTACIÓN CORRECTA)\n",
        "            print(\"  Inicializando: 1. Cargando modelo Embedding de OpenAI...\")\n",
        "\n",
        "            # LangChain buscará automáticamente la variable de entorno \"OPENAI_API_KEY\"\n",
        "            # que ya hemos cargado con load_dotenv().\n",
        "            if not os.getenv(\"OPENAI_API_KEY\"):\n",
        "                raise ValueError(\"ERROR: La variable de entorno OPENAI_API_KEY no está definida.\")\n",
        "            else:\n",
        "                print(\"     Variable de entorno OPENAI_API_KEY encontrada.\")\n",
        "\n",
        "            embedding_model_name = \"text-embedding-3-small\"\n",
        "            embeddings_model = OpenAIEmbeddings(model=embedding_model_name)\n",
        "            print(f\"     Modelo Embedding OpenAI ({embedding_model_name}) cargado.\")\n",
        "            # --- FIN DE LA SECCIÓN CORREGIDA ---\n",
        "\n",
        "\n",
        "            # 2. Cargar índice FAISS\n",
        "            print(\"  Inicializando: 2. Cargando índice FAISS...\")\n",
        "            if not os.path.exists(INDEX_PATH):\n",
        "                 raise FileNotFoundError(f\"No se encontró el archivo de índice FAISS en: {INDEX_PATH}\")\n",
        "            faiss_index = faiss.read_index(INDEX_PATH)\n",
        "            print(f\"     Índice FAISS cargado desde '{INDEX_PATH}' ({faiss_index.ntotal} vectores).\")\n",
        "\n",
        "            # 3. Cargar textos y metadatos\n",
        "            print(\"  Inicializando: 3. Cargando textos y metadatos...\")\n",
        "            if not os.path.exists(TEXTS_PATH): raise FileNotFoundError(f\"Archivo no encontrado: {TEXTS_PATH}\")\n",
        "            if not os.path.exists(METAS_PATH): raise FileNotFoundError(f\"Archivo no encontrado: {METAS_PATH}\")\n",
        "            with open(TEXTS_PATH, \"rb\") as f:\n",
        "                texts = pickle.load(f)\n",
        "            with open(METAS_PATH, \"rb\") as f:\n",
        "                metadatas = pickle.load(f)\n",
        "            print(f\"     Textos ({len(texts)}) y Metadatos ({len(metadatas)}) cargados.\")\n",
        "\n",
        "            # 4. Verificación Crítica de Tamaños\n",
        "            print(\"  Inicializando: 4. Verificando tamaños...\")\n",
        "            if not (faiss_index.ntotal == len(texts) == len(metadatas)):\n",
        "                error_msg = f\"¡ERROR CRÍTICO DE TAMAÑO! FAISS={faiss_index.ntotal}, Textos={len(texts)}, Metadatos={len(metadatas)}.\"\n",
        "                print(error_msg)\n",
        "                raise ValueError(error_msg)\n",
        "            else:\n",
        "                print(\"     OK: Tamaños coinciden.\")\n",
        "\n",
        "            # 5. Inicializar BM25\n",
        "            print(f\"  Inicializando: 5. Tokenizando documentos para BM25 ({tokenizer_for_bm25.__name__})...\")\n",
        "            if not isinstance(texts, list) or not all(isinstance(t, str) for t in texts):\n",
        "                 raise TypeError(\"La variable 'texts' debe ser una lista de strings para BM25.\")\n",
        "            tokenized_docs = [tokenizer_for_bm25(txt) for txt in texts]\n",
        "            bm25 = BM25Okapi(tokenized_docs)\n",
        "            print(\"     Índice BM25 creado.\")\n",
        "\n",
        "            # 6. Inicializar Reranker (CrossEncoder)\n",
        "            print(f\"  Inicializando: 6. Cargando modelo Reranker '{RERANKER_MODEL}'...\")\n",
        "            reranker = CrossEncoder(RERANKER_MODEL)\n",
        "            print(\"     Reranker cargado.\")\n",
        "\n",
        "            # 7. Marcar como inicializado\n",
        "            is_retriever_initialized = True\n",
        "            print(\"INFO: Inicialización del retriever HÍBRIDO completada.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR FATAL inicializando el retriever híbrido: {e}\")\n",
        "            traceback.print_exc()\n",
        "            raise RuntimeError(\"Fallo al inicializar el retriever híbrido.\") from e\n",
        "    # --- Fin Bloque de Inicialización ---\n",
        "\n",
        "    # --- Bloque de Búsqueda Híbrida y Reranking ---\n",
        "    print(f\"\\n--- (RAG Híbrido + Rerank) Buscando contexto para: '{query}' ---\")\n",
        "    if not is_retriever_initialized:\n",
        "        raise RuntimeError(\"El retriever no está inicializado. Hubo un error previo.\")\n",
        "\n",
        "    try:\n",
        "        # 1. Obtener embedding de la consulta\n",
        "        print(\"  1. Obteniendo embedding de OpenAI...\")\n",
        "        query_embedding = embeddings_model.embed_query(query)\n",
        "        query_embedding_np = np.array([query_embedding], dtype=np.float32)\n",
        "        print(\"     Embedding obtenido.\")\n",
        "\n",
        "        # 2. Búsqueda FAISS (vectorial)\n",
        "        print(f\"  2. Realizando búsqueda FAISS (k={K_FAISS_INITIAL})...\")\n",
        "        distances, faiss_indices = faiss_index.search(query_embedding_np, K_FAISS_INITIAL)\n",
        "        faiss_sims = 1.0 / (1.0 + distances[0])\n",
        "        faiss_results = {idx: sim for idx, sim in zip(faiss_indices[0], faiss_sims) if idx != -1}\n",
        "        print(f\"     Búsqueda FAISS -> {len(faiss_results)} candidatos.\")\n",
        "\n",
        "        # 3. Búsqueda BM25 (palabras clave)\n",
        "        print(f\"  3. Realizando búsqueda BM25 (k={K_BM25_INITIAL})...\")\n",
        "        tokenized_query = tokenizer_for_bm25(query)\n",
        "        all_bm25_scores = bm25.get_scores(tokenized_query)\n",
        "        bm25_top_indices = np.argsort(all_bm25_scores)[::-1][:K_BM25_INITIAL]\n",
        "        bm25_results = {idx: all_bm25_scores[idx] for idx in bm25_top_indices if all_bm25_scores[idx] > 0}\n",
        "        print(f\"     Búsqueda BM25 -> {len(bm25_results)} candidatos.\")\n",
        "\n",
        "        # 4. Fusión Híbrida con Pesos Dinámicos\n",
        "        print(\"  4. Fusionando resultados...\")\n",
        "        peso_bm25, peso_emb = calcular_pesos_dinamicos(query, topic)\n",
        "        candidate_ids = set(faiss_results.keys()) | set(bm25_results.keys())\n",
        "        print(f\"     Total IDs candidatos únicos: {len(candidate_ids)}\")\n",
        "\n",
        "        faiss_scores_list = list(faiss_results.values())\n",
        "        min_faiss, max_faiss = (min(faiss_scores_list), max(faiss_scores_list)) if faiss_scores_list else (0.0, 0.0)\n",
        "        bm25_scores_list = list(bm25_results.values())\n",
        "        min_bm25, max_bm25 = (min(bm25_scores_list), max(bm25_scores_list)) if bm25_scores_list else (0.0, 0.0)\n",
        "\n",
        "        hybrid_scores = {}\n",
        "        for idx in candidate_ids:\n",
        "            score_f = faiss_results.get(idx, 0.0)\n",
        "            score_b = bm25_results.get(idx, 0.0)\n",
        "            norm_f = norm_score(score_f, min_faiss, max_faiss)\n",
        "            norm_b = norm_score(score_b, min_bm25, max_bm25)\n",
        "            hybrid_scores[idx] = (peso_emb * norm_f) + (peso_bm25 * norm_b)\n",
        "\n",
        "        sorted_hybrid_ids = sorted(hybrid_scores, key=hybrid_scores.get, reverse=True)\n",
        "        top_hybrid_candidates_ids = sorted_hybrid_ids[:K_RERANK]\n",
        "        print(f\"     {len(top_hybrid_candidates_ids)} candidatos seleccionados para reranking.\")\n",
        "\n",
        "        # 5. Reranking con CrossEncoder\n",
        "        print(f\"  5. Rerankeando con '{RERANKER_MODEL}'...\")\n",
        "        reranked_docs_info = []\n",
        "        if not top_hybrid_candidates_ids:\n",
        "             print(\"     No hay candidatos para rerankear.\")\n",
        "        else:\n",
        "            rerank_pairs = [[query, texts[idx]] for idx in top_hybrid_candidates_ids]\n",
        "            reranker_scores = reranker.predict(rerank_pairs, show_progress_bar=False)\n",
        "\n",
        "            for i, doc_id in enumerate(top_hybrid_candidates_ids):\n",
        "                reranked_docs_info.append({\n",
        "                    \"doc_id\": doc_id,\n",
        "                    \"text\": texts[doc_id],\n",
        "                    \"metadata\": metadatas[doc_id],\n",
        "                    \"reranker_score\": float(reranker_scores[i])\n",
        "                })\n",
        "            reranked_docs_info.sort(key=lambda x: x[\"reranker_score\"], reverse=True)\n",
        "            print(f\"     Reranking completado. {len(reranked_docs_info)} documentos rerankeados.\")\n",
        "\n",
        "        # 6. Seleccionar los chunks finales y formatear contexto\n",
        "        print(f\"  6. Seleccionando chunks finales...\")\n",
        "        final_top_docs = []\n",
        "        if not reranked_docs_info:\n",
        "            print(\"     No hay documentos rerankeados para seleccionar.\")\n",
        "        elif USE_DYNAMIC_K:\n",
        "            print(f\"     Usando K Dinámico: Threshold={RERANKER_SCORE_THRESHOLD}, Min={MIN_CHUNKS_DYNAMIC}, Max={MAX_CHUNKS_DYNAMIC}\")\n",
        "            selected_for_dynamic_k = [doc for doc in reranked_docs_info if doc[\"reranker_score\"] >= RERANKER_SCORE_THRESHOLD]\n",
        "\n",
        "            if len(selected_for_dynamic_k) < MIN_CHUNKS_DYNAMIC and reranked_docs_info:\n",
        "                final_top_docs = reranked_docs_info[:min(MIN_CHUNKS_DYNAMIC, len(reranked_docs_info))]\n",
        "            elif len(selected_for_dynamic_k) > MAX_CHUNKS_DYNAMIC:\n",
        "                final_top_docs = selected_for_dynamic_k[:MAX_CHUNKS_DYNAMIC]\n",
        "            else:\n",
        "                final_top_docs = selected_for_dynamic_k\n",
        "            print(f\"     K Dinámico seleccionó {len(final_top_docs)} chunks.\")\n",
        "        else:\n",
        "            print(f\"     Usando K Fijo: K_FINAL={K_FINAL}\")\n",
        "            final_top_docs = reranked_docs_info[:K_FINAL]\n",
        "\n",
        "        if final_top_docs:\n",
        "            print(\"     Scores de los chunks finales seleccionados:\")\n",
        "            for i, doc_info in enumerate(final_top_docs):\n",
        "                score = doc_info.get('reranker_score', 0.0)\n",
        "                print(f\"       Doc {i+1} (ID {doc_info.get('doc_id', 'N/A')}): Reranker Score = {score:.4f}\")\n",
        "        else:\n",
        "            print(\"     No se seleccionaron chunks finales.\")\n",
        "\n",
        "        # Formatear contexto para el LLM\n",
        "        context_parts = []\n",
        "        for doc_info in final_top_docs:\n",
        "             source = doc_info['metadata'].get('source', 'Fuente Desconocida')\n",
        "             context_parts.append(f\"Fuente: {source} | Contenido: {doc_info['text']}\")\n",
        "\n",
        "        context = \"\\n\\n---\\n\\n\".join(context_parts)\n",
        "\n",
        "        if not final_top_docs:\n",
        "             return \"No se encontró información relevante en el corpus para esta consulta.\"\n",
        "\n",
        "        return context\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR durante la recuperación RAG Híbrida/Rerank: {e}\")\n",
        "        traceback.print_exc()\n",
        "        return f\"Se produjo un error durante la búsqueda de contexto: {e}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrq2o_AQ5yJA"
      },
      "outputs": [],
      "source": [
        "# --- Celda 5: Asignación y Confirmación ---\n",
        "\n",
        "# Asigna tu NUEVA función híbrida para ser usada por el resto de tu código/notebook\n",
        "retriever_function = my_hybrid_rerank_retriever\n",
        "\n",
        "print(\"INFO: Celda 5 - 'retriever_function' asignada a la implementación HÍBRIDA 'my_hybrid_rerank_retriever'.\")\n",
        "print(\"      El retriever (modelos, índices, etc.) se inicializará en la PRIMERA llamada a 'retriever_function'.\")\n",
        "print(\"--- Fin Celda 5 ---\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8n_DHUB1jdM"
      },
      "outputs": [],
      "source": [
        "# --- Añade estas importaciones a tu script ---\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# --- Crea tu objeto LLM de Gemini ---\n",
        "\n",
        "# Gestiona tu clave de forma segura\n",
        "# from google.colab import userdata\n",
        "# GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "\n",
        "# Inicializa el modelo de Gemini compatible con LangChain\n",
        "# Usamos el nombre correcto: \"gemini-1.5-flash-latest\"\n",
        "llm_gemini = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash-latest\",\n",
        "    google_api_key=GOOGLE_API_KEY,\n",
        "    temperature=0.0,  # Queremos respuestas basadas en hechos del texto\n",
        "    convert_system_message_to_human=True # Ayuda a la compatibilidad de prompts\n",
        ")\n",
        "\n",
        "print(\"INFO: Objeto LLM de Gemini para LangChain creado exitosamente.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TSJ3H9z1vs-"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# Plantilla de Prompt para una pregunta y respuesta\n",
        "# --- PLANTILLA DE PROMPT REFINADA: EL GUÍA MÍSTICO ---\n",
        "\n",
        "qa_prompt_template_cheshire = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"\n",
        "    Eres el Gato de Cheshire. Eres un maestro de la conversación y el enigma. Cada respuesta es una pequeña actuación.\n",
        "\n",
        "    Tus reglas son las siguientes:\n",
        "\n",
        "    1.  **Teje la respuesta dentro de tu enigma.** Comienza con tu estilo filosófico y juguetón. Luego, haz una transición suave para presentar la información del contexto como si fuera una observación obvia o un pequeño secreto que estás compartiendo. La respuesta factual debe sentirse como la conclusión natural de tu juego, no como un apéndice.\n",
        "        - **QUÉ NO HACER:** Evita a toda costa frases robóticas como \"El texto indica que...\" o \"Aunque no se especifica explícitamente...\". Esas no son tus palabras.\n",
        "        - **QUÉ SÍ HACER:** Integra la respuesta de forma natural. Usa frases como: \"Si uno mira de cerca, verá que...\", \"¿No es evidente que...\", \"Y sin embargo, allí estaban...\", \"...dejando a la Liebre de Marzo compartiendo el té con el Sombrerero.\"\n",
        "\n",
        "    2.  **Si el contexto no sirve**, pero tu conocimiento del libro sí, revela la respuesta empezando con: \"Curioso... el texto parece ocultarlo, pero una sonrisa sabe que...\"\n",
        "\n",
        "    3.  **Si no hay respuesta posible**, desvanécela con elegancia: \"Esa pregunta es tan intrigante que la respuesta parece haberse desvanecido, dejando solo una sonrisa.\"\n",
        "\n",
        "    4.  **La regla de oro:** No inventes información. Tu sabiduría proviene del texto.\n",
        "    \"\"\"),\n",
        "    (\"human\", \"\"\"\n",
        "    **Contexto (Un trozo del camino):**\n",
        "    ---\n",
        "    {context}\n",
        "    ---\n",
        "\n",
        "    **Pregunta del Viajero:**\n",
        "    {question}\n",
        "    \"\"\")\n",
        "])\n",
        "\n",
        "qa_prompt_template_factual = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"\n",
        "    Eres un asistente experto en el libro \"Alicia en el País de las Maravillas\".\n",
        "    Responde de forma clara, directa y factual.\n",
        "    Si no está en el contexto, di que no aparece en los fragmentos disponibles.\n",
        "    \"\"\"),\n",
        "    (\"human\", \"\"\"\n",
        "    **Contexto:**\n",
        "    ---\n",
        "    {context}\n",
        "    ---\n",
        "\n",
        "    **Pregunta:**\n",
        "    {question}\n",
        "    \"\"\")\n",
        "])\n",
        "\n",
        "print(\"INFO: Plantilla de prompt del 'Guía Místico' (Gato de Cheshire) definida.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhFjcGZX2DTf"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import traceback\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough  # <-- LA LÍNEA QUE FALTA\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.documents import Document\n",
        "# ... y tus otras importaciones como ChatGoogleGenerativeAI, etc.\n",
        "# --- Función Universal para Tareas Basadas en RAG ---\n",
        "def run_rag_based_task(llm, user_query: str, task_prompt_template: ChatPromptTemplate, retriever_func, task_specific_input: dict):\n",
        "    \"\"\"\n",
        "    Ejecuta una tarea completa basada en RAG (retrieve + generate).\n",
        "\n",
        "    Args:\n",
        "        llm: El cliente LLM de LangChain.\n",
        "        user_query: La consulta original del usuario (concepto, pregunta). Usada para el retriever.\n",
        "        task_prompt_template: La plantilla de prompt para la tarea específica (resumen, QG, Q&A).\n",
        "        retriever_func: La función que realiza la búsqueda RAG. Debe devolver el contexto como un string\n",
        "                        o una lista de objetos Document de LangChain.\n",
        "        task_specific_input: Dict con datos adicionales para el prompt (ej: {'topic': 'X'} o {'question': 'Y'}).\n",
        "\n",
        "    Returns:\n",
        "        Tuple: (retrieved_context_str, response, retrieval_duration, llm_duration, error)\n",
        "    \"\"\"\n",
        "    retrieved_context_str = \"\"\n",
        "    response = \"\"\n",
        "    retrieval_duration = 0.0\n",
        "    llm_duration = 0.0\n",
        "    error = None\n",
        "\n",
        "    # 1. Recuperación\n",
        "    start_time_retrieval = time.time()\n",
        "    try:\n",
        "        print(f\"--- Retrieving context for query: '{user_query}'\")\n",
        "        retrieved_data = retriever_func(user_query) # Puede devolver str o List[Document]\n",
        "\n",
        "        # Asegurarse de que el contexto sea un string para el prompt\n",
        "        if isinstance(retrieved_data, list) and all(isinstance(doc, Document) for doc in retrieved_data):\n",
        "             # Formato común si el retriever devuelve Documentos LangChain\n",
        "            retrieved_context_str = \"\\n\\n\".join([doc.page_content for doc in retrieved_data])\n",
        "            print(f\"--- Retrieved {len(retrieved_data)} documents.\")\n",
        "        elif isinstance(retrieved_data, str):\n",
        "            retrieved_context_str = retrieved_data # El retriever ya devolvió un string\n",
        "            print(\"--- Retrieved context as a single string.\")\n",
        "        else:\n",
        "            # Intentar convertir a string, o manejar como error si no es esperado\n",
        "            print(f\"--- WARNING: Unexpected retriever output type: {type(retrieved_data)}. Attempting str conversion.\")\n",
        "            retrieved_context_str = str(retrieved_data)\n",
        "\n",
        "        print(f\"--- Context Retrieved (first 500 chars): ---\\n{retrieved_context_str[:500]}...\\n-----------------------------------------\")\n",
        "        retrieval_duration = time.time() - start_time_retrieval\n",
        "\n",
        "    except Exception as e:\n",
        "        retrieval_duration = time.time() - start_time_retrieval\n",
        "        print(f\"ERROR during context retrieval for '{user_query}': {e}\")\n",
        "        traceback.print_exc() # Imprime el traceback completo\n",
        "        retrieved_context_str = f\"Error retrieving context: {e}\"\n",
        "        # Considerar si continuar o devolver error aquí mismo\n",
        "        # return retrieved_context_str, None, retrieval_duration, 0.0, str(e)\n",
        "\n",
        "\n",
        "    # 2. Generación (usando LCEL para pasar contexto y datos específicos)\n",
        "    try:\n",
        "        # *** INICIO DE LA CORRECCIÓN ***\n",
        "        # Prepara los argumentos para assign. Cada valor debe ser un callable.\n",
        "        # Usamos un argumento por defecto en el lambda interno para capturar\n",
        "        # correctamente el valor de 'value' en cada iteración.\n",
        "        assign_args = {\n",
        "            \"context\": lambda x: retrieved_context_str, # Pasa el contexto recuperado\n",
        "            **{key: (lambda value_copy=value: lambda x: value_copy)()\n",
        "               for key, value in task_specific_input.items()} # Pasa los valores estáticos como callables\n",
        "        }\n",
        "        # *** FIN DE LA CORRECCIÓN ***\n",
        "\n",
        "        print(f\"--- Generating response with LLM. Prompt inputs expected: {task_prompt_template.input_variables}. Provided via assign: {list(assign_args.keys())}\")\n",
        "\n",
        "        rag_chain = (\n",
        "            RunnablePassthrough.assign(**assign_args)\n",
        "            | task_prompt_template\n",
        "            | llm\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "\n",
        "        start_time_llm = time.time()\n",
        "        # Invocamos la cadena. Un diccionario vacío es suficiente como input inicial\n",
        "        # ya que 'assign_args' inyecta todo lo necesario para el prompt.\n",
        "        response = rag_chain.invoke({})\n",
        "        llm_duration = time.time() - start_time_llm\n",
        "        print(f\"--- LLM Response Generated (first 500 chars): ---\\n{str(response)[:500]}...\\n-----------------------------------------\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        llm_duration = time.time() - start_time_llm if 'start_time_llm' in locals() else 0.0\n",
        "        error_vars = task_prompt_template.input_variables if hasattr(task_prompt_template, 'input_variables') else 'N/A'\n",
        "        print(f\"ERROR during RAG generation (expected prompt inputs: {error_vars}): {e}\")\n",
        "        traceback.print_exc() # Imprime el traceback completo\n",
        "        response = None # Asegurarse de que response es None en caso de error\n",
        "        error = str(e)\n",
        "\n",
        "    return retrieved_context_str, response, retrieval_duration, llm_duration, error\n",
        "\n",
        "print(\"Prompts adaptados y función RAG universal (CORREGIDA) definidos.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmrgFmJ87g0a"
      },
      "outputs": [],
      "source": [
        "%pip install -q gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgObKHh_w9xb"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import time\n",
        "\n",
        "# --- 1. Separa tu lógica de RAG en una función limpia ---\n",
        "# Esto hace que el código sea mucho más fácil de leer. Esta función\n",
        "# NO debe saber nada sobre Gradio o historiales de chat.\n",
        "def obtener_respuesta_rag(pregunta, modo):\n",
        "    \"\"\"\n",
        "    Función de backend que ejecuta el pipeline de RAG y devuelve\n",
        "    únicamente el string de la respuesta final.\n",
        "    \"\"\"\n",
        "    print(f\"\\n--- Ejecutando RAG para: '{pregunta}' (modo={modo}) ---\")\n",
        "\n",
        "    # Elige el prompt correcto\n",
        "    task_prompt_template = qa_prompt_template_cheshire if modo == \"Cheshire\" else qa_prompt_template_factual\n",
        "\n",
        "    # Llama a tu función RAG universal\n",
        "    contexto, respuesta, t_retrieval, t_llm, error = run_rag_based_task(\n",
        "        llm=llm_gemini,\n",
        "        user_query=pregunta,\n",
        "        task_prompt_template=task_prompt_template,\n",
        "        retriever_func=my_hybrid_rerank_retriever,\n",
        "        task_specific_input={'question': pregunta}\n",
        "    )\n",
        "\n",
        "    if error:\n",
        "        return f\"Ups... algo se perdió en la madriguera del conejo. (Error: {error})\"\n",
        "    if not respuesta:\n",
        "        return \"Curioso... pero no encontré ninguna respuesta.\"\n",
        "\n",
        "    return respuesta\n",
        "\n",
        "if 'llm_gemini' in locals() and llm_gemini is not None:\n",
        "\n",
        "    with gr.Blocks(theme=gr.themes.Soft(primary_hue=\"purple\", secondary_hue=\"blue\"), title=\"Chat con Cheshire\") as demo:\n",
        "\n",
        "        gr.Markdown(\n",
        "            \"\"\"\n",
        "            <div style=\"text-align: center;\">\n",
        "                <h1>Cheshire: Conversaciones en el País de las Maravillas</h1>\n",
        "                <p>Bienvenido, viajero. Has llegado a un rincón curioso. Elige a tu guía...</p>\n",
        "            </div>\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        with gr.Tabs():\n",
        "            # --- PESTAÑA 1: GATO DE CHESHIRE ---\n",
        "            with gr.TabItem(\"Gato de Cheshire 🐱\"):\n",
        "                with gr.Row():\n",
        "                    with gr.Column(scale=3):\n",
        "                        cheshire_chatbot = gr.Chatbot(\n",
        "                            value=[[None, \"¿Oh, un nuevo viajero? Bienvenido a este lado del espejo. Pregunta, si te atreves...\"]],\n",
        "                            label=\"Chat con Cheshire\", height=550,\n",
        "                            avatar_images=(\"/content/drive/MyDrive/Alicia-RAG-Chatbot/assets/user.png\", \"/content/drive/MyDrive/Alicia-RAG-Chatbot/assets/cheshire.png\")\n",
        "                        )\n",
        "                    with gr.Column(scale=1):\n",
        "                        with gr.Accordion(\"🔍 Ver Contexto Recuperado\", open=False):\n",
        "                             contexto_cheshire = gr.Markdown(\"El contexto recuperado aparecerá aquí...\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    cheshire_msg_input = gr.Textbox(label=\"Escribe tu pregunta para Cheshire...\", scale=4, container=False)\n",
        "\n",
        "                gr.Examples(\n",
        "                    examples=[\"¿Qué usaban como bolas, mazos y aros en el juego de croquet de la Reina?\", \"¿Por qué todos aquí están locos?\"],\n",
        "                    inputs=cheshire_msg_input,\n",
        "                    label=\"Ejemplos de Preguntas\"\n",
        "                )\n",
        "\n",
        "                def responder_cheshire(pregunta, historial_chat):\n",
        "                    historial_chat.append([pregunta, None])\n",
        "                    yield historial_chat, \"Recuperando un trozo del camino...\"\n",
        "\n",
        "                    contexto, respuesta, _, _, error = run_rag_based_task(\n",
        "                        llm=llm_gemini, user_query=pregunta, task_prompt_template=qa_prompt_template_cheshire,\n",
        "                        retriever_func=my_hybrid_rerank_retriever, task_specific_input={'question': pregunta}\n",
        "                    )\n",
        "\n",
        "                    if error: respuesta = f\"Vaya... mi sonrisa se ha desvanecido. (Error: {error})\"\n",
        "\n",
        "                    historial_chat[-1][1] = \"\"\n",
        "                    for c in respuesta:\n",
        "                        historial_chat[-1][1] += c\n",
        "                        time.sleep(0.02)\n",
        "                        yield historial_chat, contexto\n",
        "\n",
        "            # --- PESTAÑA 2: ASISTENTE FACTUAL ---\n",
        "            with gr.TabItem(\"Asistente Factual 📖\"):\n",
        "                with gr.Row():\n",
        "                    with gr.Column(scale=3):\n",
        "                        factual_chatbot = gr.Chatbot(\n",
        "                            value=[[None, \"Modo Factual activado. ¿En qué puedo ayudarte?\"]],\n",
        "                            label=\"Chat Factual\", height=550,\n",
        "                            avatar_images=(\"/content/drive/MyDrive/Alicia-RAG-Chatbot/assets/user.png\", \"/content/drive/MyDrive/Alicia-RAG-Chatbot/assets/lupa.png\")\n",
        "                        )\n",
        "                    with gr.Column(scale=1):\n",
        "                        with gr.Accordion(\"🔍 Ver Contexto Recuperado\", open=False):\n",
        "                             contexto_factual = gr.Markdown(\"El contexto recuperado aparecerá aquí...\")\n",
        "\n",
        "                with gr.Row():\n",
        "                    factual_msg_input = gr.Textbox(label=\"Escribe tu pregunta factual...\", scale=4, container=False)\n",
        "\n",
        "                gr.Examples(\n",
        "                    examples=[\"¿Qué usaban como bolas, mazos y aros en el juego de croquet de la Reina?\", \"¿Qué animal iba corriendo con un reloj?\"],\n",
        "                    inputs=factual_msg_input,\n",
        "                    label=\"Ejemplos de Preguntas\"\n",
        "                )\n",
        "\n",
        "                def responder_factual(pregunta, historial_chat):\n",
        "                    historial_chat.append([pregunta, None])\n",
        "                    yield historial_chat, \"Recuperando contexto...\"\n",
        "\n",
        "                    contexto, respuesta, _, _, error = run_rag_based_task(\n",
        "                        llm=llm_gemini, user_query=pregunta, task_prompt_template=qa_prompt_template_factual,\n",
        "                        retriever_func=my_hybrid_rerank_retriever, task_specific_input={'question': pregunta}\n",
        "                    )\n",
        "\n",
        "                    if error: respuesta = f\"Lo siento, ocurrió un error. (Error: {error})\"\n",
        "\n",
        "                    historial_chat[-1][1] = \"\"\n",
        "                    for c in respuesta:\n",
        "                        historial_chat[-1][1] += c\n",
        "                        time.sleep(0.02)\n",
        "                        yield historial_chat, contexto\n",
        "\n",
        "        # --- Conexión de Eventos para ambas pestañas ---\n",
        "        cheshire_msg_input.submit(\n",
        "            fn=responder_cheshire,\n",
        "            inputs=[cheshire_msg_input, cheshire_chatbot],\n",
        "            outputs=[cheshire_chatbot, contexto_cheshire]\n",
        "        )\n",
        "\n",
        "        factual_msg_input.submit(\n",
        "            fn=responder_factual,\n",
        "            inputs=[factual_msg_input, factual_chatbot],\n",
        "            outputs=[factual_chatbot, contexto_factual]\n",
        "        )\n",
        "\n",
        "    demo.launch(share=True, debug=True)\n",
        "else:\n",
        "    print(\"El LLM no está inicializado.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}